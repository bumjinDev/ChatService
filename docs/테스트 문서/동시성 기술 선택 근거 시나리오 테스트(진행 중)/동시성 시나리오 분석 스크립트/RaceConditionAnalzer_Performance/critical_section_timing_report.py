import pandas as pd
import numpy as np
import os
from datetime import datetime

def read_critical_section_logs(input_file):
    """
    CRITICAL_SECTION_MARK ë¡œê·¸ íŒŒì¼ ì½ê¸°
    """
    if not os.path.exists(input_file):
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return pd.DataFrame()
    
    df = pd.read_csv(input_file)
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    return df

def read_increment_logs(input_file):
    """
    INCREMENT ë¡œê·¸ íŒŒì¼ ì½ê¸°
    """
    if not os.path.exists(input_file):
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return pd.DataFrame()
    
    df = pd.read_csv(input_file, parse_dates=['increment_start_time', 'increment_end_time'])
    return df

def analyze_critical_section_timing(df_critical):
    """
    WAITING_START â†’ CRITICAL_ENTER â†’ CRITICAL_LEAVE êµ¬ê°„ë³„ ì‹œê°„ ë¶„ì„
    """
    if df_critical.empty:
        return pd.DataFrame(), {}
    
    timing_result = []
    stats = {
        'total_sessions': 0,
        'complete_sessions': 0,
        'waiting_stats': {},
        'critical_stats': {},
        'total_stats': {},
        'concurrency_stats': {}
    }
    
    # ì‚¬ìš©ìë³„ ê·¸ë£¹í™”
    grouped = df_critical.groupby(['roomNumber', 'userId'])
    stats['total_sessions'] = len(grouped)
    
    for (room, user), group in grouped:
        # ê° íƒœê·¸ë³„ ì‹œê°„ ì¶”ì¶œ
        marks = {}
        for tag in ['WAITING_START', 'CRITICAL_ENTER', 'CRITICAL_LEAVE']:
            sub = group[group['tag'] == tag]
            if not sub.empty:
                row = sub.iloc[0]
                marks[tag] = {
                    'timestamp': row['timestamp'],
                    'nanoTime': row.get('nanoTime', None),
                    'epochNano': row.get('epochNano', None),
                    'threadId': row.get('threadId', None)
                }
        
        # ì™„ì „í•œ ì„¸ì…˜ì¸ì§€ í™•ì¸
        if set(marks.keys()) == {'WAITING_START', 'CRITICAL_ENTER', 'CRITICAL_LEAVE'}:
            stats['complete_sessions'] += 1
            
            # ì‹œê°„ ê³„ì‚°
            t_wait = marks['WAITING_START']['timestamp']
            t_enter = marks['CRITICAL_ENTER']['timestamp']
            t_leave = marks['CRITICAL_LEAVE']['timestamp']
            
            waiting_time_ms = (t_enter - t_wait).total_seconds() * 1000
            critical_time_ms = (t_leave - t_enter).total_seconds() * 1000
            total_time_ms = (t_leave - t_wait).total_seconds() * 1000
            
            # ë‚˜ë…¸ì´ˆ ì •ë°€ë„ ê³„ì‚°
            nano_waiting_ns = None
            nano_critical_ns = None
            nano_total_ns = None
            
            if all(marks[tag].get('nanoTime') for tag in marks):
                nano_waiting_ns = marks['CRITICAL_ENTER']['nanoTime'] - marks['WAITING_START']['nanoTime']
                nano_critical_ns = marks['CRITICAL_LEAVE']['nanoTime'] - marks['CRITICAL_ENTER']['nanoTime']
                nano_total_ns = marks['CRITICAL_LEAVE']['nanoTime'] - marks['WAITING_START']['nanoTime']
            
            timing_result.append({
                'roomNumber': room,
                'userId': user,
                'waiting_time_ms': waiting_time_ms,
                'critical_section_ms': critical_time_ms,
                'total_time_ms': total_time_ms,
                'waiting_time_ns': nano_waiting_ns,
                'critical_section_ns': nano_critical_ns,
                'total_time_ns': nano_total_ns,
                't_wait': t_wait,
                't_enter': t_enter,
                't_leave': t_leave,
                'thread_wait': marks['WAITING_START'].get('threadId'),
                'thread_enter': marks['CRITICAL_ENTER'].get('threadId'),
                'thread_leave': marks['CRITICAL_LEAVE'].get('threadId'),
                'thread_consistency': (
                    marks['WAITING_START'].get('threadId') == 
                    marks['CRITICAL_ENTER'].get('threadId') == 
                    marks['CRITICAL_LEAVE'].get('threadId')
                )
            })
    
    timing_df = pd.DataFrame(timing_result)
    
    if not timing_df.empty:
        # í†µê³„ ê³„ì‚°
        for phase, col in [('waiting', 'waiting_time_ms'), 
                          ('critical', 'critical_section_ms'), 
                          ('total', 'total_time_ms')]:
            data = timing_df[col]
            stats[f'{phase}_stats'] = {
                'count': len(data),
                'mean': data.mean(),
                'median': data.median(),
                'std': data.std(),
                'min': data.min(),
                'max': data.max(),
                'q25': data.quantile(0.25),
                'q75': data.quantile(0.75),
                'q95': data.quantile(0.95),
                'q99': data.quantile(0.99)
            }
        
        # ë™ì‹œì„± í†µê³„
        stats['concurrency_stats'] = {
            'unique_threads': timing_df['thread_wait'].nunique(),
            'thread_consistency_rate': timing_df['thread_consistency'].mean() * 100,
            'avg_threads_per_room': timing_df.groupby('roomNumber')['thread_wait'].nunique().mean()
        }
    
    return timing_df, stats

def analyze_increment_timing(df_increment):
    """
    INCREMENT_BEFORE â†’ INCREMENT_AFTER êµ¬ê°„ ë¶„ì„
    """
    if df_increment.empty:
        return pd.DataFrame(), {}
    
    # ì‹œê°„ ê³„ì‚°
    df_increment = df_increment.copy()
    df_increment['increment_duration_ms'] = (
        df_increment['increment_end_time'] - df_increment['increment_start_time']
    ).dt.total_seconds() * 1000
    
    # ë‚˜ë…¸ì´ˆ ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
    if 'increment_nanoTime_start' in df_increment.columns and 'increment_nanoTime_end' in df_increment.columns:
        df_increment['increment_duration_ns'] = (
            df_increment['increment_nanoTime_end'] - df_increment['increment_nanoTime_start']
        )
    
    # í†µê³„ ê³„ì‚°
    duration_data = df_increment['increment_duration_ms']
    stats = {
        'total_operations': len(df_increment),
        'rooms_count': df_increment['roomNumber'].nunique(),
        'users_count': df_increment['user_id'].nunique(),
        'threads_count': df_increment['thread_id_start'].nunique(),
        
        # ì‹œê°„ í†µê³„
        'duration_stats_ms': {
            'mean': duration_data.mean(),
            'median': duration_data.median(),
            'std': duration_data.std(),
            'min': duration_data.min(),
            'max': duration_data.max(),
            'q25': duration_data.quantile(0.25),
            'q75': duration_data.quantile(0.75),
            'q95': duration_data.quantile(0.95),
            'q99': duration_data.quantile(0.99)
        },
        
        # ì •í™•ì„± í†µê³„
        'accuracy_stats': {
            'normal_increments': len(df_increment[df_increment['people_increment'] == 1]),
            'abnormal_increments': len(df_increment[df_increment['people_increment'] != 1]),
            'accuracy_rate': (df_increment['people_increment'] == 1).mean() * 100,
            'increment_distribution': df_increment['people_increment'].value_counts().to_dict()
        },
        
        # ì²˜ë¦¬ëŸ‰ í†µê³„
        'throughput_stats': {
            'operations_per_room': df_increment.groupby('roomNumber').size().to_dict(),
            'operations_per_thread': df_increment.groupby('thread_id_start').size().to_dict(),
            'avg_operations_per_room': df_increment.groupby('roomNumber').size().mean(),
            'avg_operations_per_thread': df_increment.groupby('thread_id_start').size().mean()
        },
        
        # ë¶€í•˜ë³„ ì„±ëŠ¥ (êµ¬ê°„ë³„)
        'load_performance': {},
        
        # ìŠ¤ë ˆë“œ ì¼ê´€ì„±
        'thread_consistency': {
            'same_thread_operations': len(df_increment[
                df_increment['thread_id_start'] == df_increment['thread_id_end']
            ]),
            'thread_switch_operations': len(df_increment[
                df_increment['thread_id_start'] != df_increment['thread_id_end']
            ]),
            'thread_consistency_rate': (
                df_increment['thread_id_start'] == df_increment['thread_id_end']
            ).mean() * 100
        }
    }
    
    # êµ¬ê°„ë³„(ë¶€í•˜ë³„) ì„±ëŠ¥ ë¶„ì„
    if 'bin' in df_increment.columns:
        bin_stats = df_increment.groupby('bin').agg({
            'increment_duration_ms': ['count', 'mean', 'median', 'std'],
            'people_increment': lambda x: (x == 1).mean() * 100,
            'thread_id_start': 'nunique'
        }).round(3)
        
        bin_stats.columns = ['operations', 'avg_time_ms', 'median_time_ms', 'std_time_ms', 
                            'accuracy_rate', 'thread_count']
        stats['load_performance'] = bin_stats.to_dict('index')
    
    # ë‚˜ë…¸ì´ˆ í†µê³„ (ê°€ëŠ¥í•œ ê²½ìš°)
    if 'increment_duration_ns' in df_increment.columns:
        nano_data = df_increment['increment_duration_ns'].dropna()
        if not nano_data.empty:
            stats['duration_stats_ns'] = {
                'mean': nano_data.mean(),
                'median': nano_data.median(),
                'std': nano_data.std(),
                'min': nano_data.min(),
                'max': nano_data.max(),
                'q25': nano_data.quantile(0.25),
                'q75': nano_data.quantile(0.75)
            }
    
    return df_increment, stats

def generate_comprehensive_report(critical_timing, critical_stats, increment_timing, increment_stats, tech_name):
    """
    ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    """
    report_lines = []
    report_lines.append(f"=== {tech_name.upper()} ë™ì‹œì„± ì œì–´ ê¸°ìˆ  ë¶„ì„ ë¦¬í¬íŠ¸ ===")
    report_lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # 1. ì „ì²´ ê°œìš”
    report_lines.append("ğŸ“Š ì „ì²´ ê°œìš”")
    report_lines.append("-" * 50)
    if critical_stats:
        report_lines.append(f"ì„ê³„êµ¬ì—­ ì„¸ì…˜ ìˆ˜: {critical_stats['complete_sessions']} / {critical_stats['total_sessions']}")
        completion_rate = critical_stats['complete_sessions'] / critical_stats['total_sessions'] * 100
        report_lines.append(f"ì™„ë£Œìœ¨: {completion_rate:.1f}%")
    
    if increment_stats:
        report_lines.append(f"ì¦ê°€ ì‘ì—… ìˆ˜: {increment_stats['total_operations']}")
        report_lines.append(f"ì²˜ë¦¬ëœ ë°© ìˆ˜: {increment_stats['rooms_count']}")
        report_lines.append(f"í™œì„± ìŠ¤ë ˆë“œ ìˆ˜: {increment_stats['threads_count']}")
        report_lines.append(f"ì •í™•ì„±: {increment_stats['accuracy_stats']['accuracy_rate']:.1f}%")
    report_lines.append("")
    
    # 2. ëŒ€ê¸° ì‹œê°„ ë¶„ì„ (WAITING_START â†’ CRITICAL_ENTER)
    if critical_stats and 'waiting_stats' in critical_stats:
        report_lines.append("â° ëŒ€ê¸° ì‹œê°„ ë¶„ì„ (WAITING â†’ ENTER)")
        report_lines.append("-" * 50)
        ws = critical_stats['waiting_stats']
        report_lines.append(f"í‰ê·  ëŒ€ê¸° ì‹œê°„: {ws['mean']:.3f} ms")
        report_lines.append(f"ì¤‘ì•™ê°’ ëŒ€ê¸° ì‹œê°„: {ws['median']:.3f} ms")
        report_lines.append(f"ìµœëŒ€ ëŒ€ê¸° ì‹œê°„: {ws['max']:.3f} ms")
        report_lines.append(f"95% ëŒ€ê¸° ì‹œê°„: {ws['q95']:.3f} ms")
        report_lines.append(f"99% ëŒ€ê¸° ì‹œê°„: {ws['q99']:.3f} ms")
        report_lines.append("")
    
    # 3. ì„ê³„êµ¬ì—­ ì‹¤í–‰ ì‹œê°„ ë¶„ì„ (CRITICAL_ENTER â†’ CRITICAL_LEAVE)
    if critical_stats and 'critical_stats' in critical_stats:
        report_lines.append("ğŸ”’ ì„ê³„êµ¬ì—­ ì‹¤í–‰ ì‹œê°„ ë¶„ì„ (ENTER â†’ LEAVE)")
        report_lines.append("-" * 50)
        cs = critical_stats['critical_stats']
        report_lines.append(f"í‰ê·  ì‹¤í–‰ ì‹œê°„: {cs['mean']:.3f} ms")
        report_lines.append(f"ì¤‘ì•™ê°’ ì‹¤í–‰ ì‹œê°„: {cs['median']:.3f} ms")
        report_lines.append(f"ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {cs['max']:.3f} ms")
        report_lines.append(f"í‘œì¤€í¸ì°¨: {cs['std']:.3f} ms")
        report_lines.append(f"95% ì‹¤í–‰ ì‹œê°„: {cs['q95']:.3f} ms")
        report_lines.append("")
    
    # 4. ì¦ê°€ ì‘ì—… ì‹œê°„ ë¶„ì„ (INCREMENT_BEFORE â†’ INCREMENT_AFTER)
    if increment_stats and 'duration_stats_ms' in increment_stats:
        report_lines.append("âš¡ ì¦ê°€ ì‘ì—… ì‹œê°„ ë¶„ì„ (INCREMENT_BEFORE â†’ INCREMENT_AFTER)")
        report_lines.append("-" * 50)
        ds = increment_stats['duration_stats_ms']
        report_lines.append(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {ds['mean']:.3f} ms")
        report_lines.append(f"ì¤‘ì•™ê°’ ì²˜ë¦¬ ì‹œê°„: {ds['median']:.3f} ms")
        report_lines.append(f"ìµœì†Œ/ìµœëŒ€ ì‹œê°„: {ds['min']:.3f} / {ds['max']:.3f} ms")
        report_lines.append(f"í‘œì¤€í¸ì°¨: {ds['std']:.3f} ms")
        report_lines.append(f"95% ì²˜ë¦¬ ì‹œê°„: {ds['q95']:.3f} ms")
        report_lines.append(f"99% ì²˜ë¦¬ ì‹œê°„: {ds['q99']:.3f} ms")
        
        # ë‚˜ë…¸ì´ˆ ì •ë°€ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
        if 'duration_stats_ns' in increment_stats:
            ns = increment_stats['duration_stats_ns']
            report_lines.append(f"ë‚˜ë…¸ì´ˆ í‰ê· : {ns['mean']:.0f} ns")
            report_lines.append(f"ë‚˜ë…¸ì´ˆ ì¤‘ì•™ê°’: {ns['median']:.0f} ns")
        report_lines.append("")
    
    # 5. ë™ì‹œì„± ì •í™•ì„± ë¶„ì„
    if increment_stats and 'accuracy_stats' in increment_stats:
        report_lines.append("âœ… ë™ì‹œì„± ì •í™•ì„± ë¶„ì„")
        report_lines.append("-" * 50)
        acc = increment_stats['accuracy_stats']
        report_lines.append(f"ì •ìƒ ì¦ê°€ (1): {acc['normal_increments']}ê±´")
        report_lines.append(f"ë¹„ì •ìƒ ì¦ê°€: {acc['abnormal_increments']}ê±´")
        report_lines.append(f"ì •í™•ì„± ë¹„ìœ¨: {acc['accuracy_rate']:.2f}%")
        
        if acc['abnormal_increments'] > 0:
            report_lines.append("ì¦ê°€ëŸ‰ë³„ ë¶„í¬:")
            for increment, count in acc['increment_distribution'].items():
                report_lines.append(f"  ì¦ê°€ëŸ‰ {increment}: {count}ê±´")
        report_lines.append("")
    
    # 6. ìŠ¤ë ˆë“œ í™œìš© ë¶„ì„
    if increment_stats and 'thread_consistency' in increment_stats:
        report_lines.append("ğŸ§µ ìŠ¤ë ˆë“œ í™œìš© ë¶„ì„")
        report_lines.append("-" * 50)
        tc = increment_stats['thread_consistency']
        tp = increment_stats['throughput_stats']
        
        report_lines.append(f"ì´ í™œì„± ìŠ¤ë ˆë“œ: {increment_stats['threads_count']}")
        report_lines.append(f"ìŠ¤ë ˆë“œ ì¼ê´€ì„±: {tc['thread_consistency_rate']:.1f}%")
        report_lines.append(f"ë™ì¼ ìŠ¤ë ˆë“œ ì‘ì—…: {tc['same_thread_operations']}ê±´")
        report_lines.append(f"ìŠ¤ë ˆë“œ ì „í™˜ ì‘ì—…: {tc['thread_switch_operations']}ê±´")
        report_lines.append(f"ìŠ¤ë ˆë“œë‹¹ í‰ê·  ì‘ì—…: {tp['avg_operations_per_thread']:.1f}ê±´")
        report_lines.append("")
    
    # 7. ë¶€í•˜ë³„ ì„±ëŠ¥ ë¶„ì„ (êµ¬ê°„ë³„)
    if increment_stats and 'load_performance' in increment_stats and increment_stats['load_performance']:
        report_lines.append("ğŸ“ˆ ë¶€í•˜ë³„ ì„±ëŠ¥ ë¶„ì„ (10ê°œ êµ¬ê°„)")
        report_lines.append("-" * 50)
        report_lines.append("êµ¬ê°„ | ì‘ì—…ìˆ˜ | í‰ê· ì‹œê°„(ms) | ì •í™•ì„±(%) | ìŠ¤ë ˆë“œìˆ˜")
        report_lines.append("-" * 50)
        
        for bin_num, stats in increment_stats['load_performance'].items():
            report_lines.append(f"{bin_num:3d}  | {stats['operations']:5.0f} | {stats['avg_time_ms']:9.3f} | {stats['accuracy_rate']:7.1f} | {stats['thread_count']:6.0f}")
        report_lines.append("")
    
    # 8. ì²˜ë¦¬ëŸ‰ ë¶„ì„
    if increment_stats and 'throughput_stats' in increment_stats:
        report_lines.append("ğŸš€ ì²˜ë¦¬ëŸ‰ ë¶„ì„")
        report_lines.append("-" * 50)
        tp = increment_stats['throughput_stats']
        report_lines.append(f"ë°©ë‹¹ í‰ê·  ì‘ì—…: {tp['avg_operations_per_room']:.1f}ê±´")
        report_lines.append(f"ìŠ¤ë ˆë“œë‹¹ í‰ê·  ì‘ì—…: {tp['avg_operations_per_thread']:.1f}ê±´")
        
        # ë°©ë³„ ì²˜ë¦¬ëŸ‰ (ìƒìœ„ 5ê°œ)
        room_ops = sorted(tp['operations_per_room'].items(), key=lambda x: x[1], reverse=True)[:5]
        report_lines.append("ë°©ë³„ ì²˜ë¦¬ëŸ‰ (ìƒìœ„ 5ê°œ):")
        for room, ops in room_ops:
            report_lines.append(f"  ë°© {room}: {ops}ê±´")
        report_lines.append("")
    
    # 9. ì„±ëŠ¥ ìš”ì•½ ë° ê¶Œì¥ì‚¬í•­
    report_lines.append("ğŸ“‹ ì„±ëŠ¥ ìš”ì•½")
    report_lines.append("-" * 50)
    
    if increment_stats and 'duration_stats_ms' in increment_stats:
        avg_time = increment_stats['duration_stats_ms']['mean']
        accuracy = increment_stats['accuracy_stats']['accuracy_rate']
        
        # ì„±ëŠ¥ ë“±ê¸‰ ë§¤ê¸°ê¸°
        if avg_time < 1.0 and accuracy > 99.0:
            grade = "A+ (ë§¤ìš° ìš°ìˆ˜)"
        elif avg_time < 2.0 and accuracy > 95.0:
            grade = "A (ìš°ìˆ˜)"
        elif avg_time < 5.0 and accuracy > 90.0:
            grade = "B (ì–‘í˜¸)"
        elif avg_time < 10.0 and accuracy > 80.0:
            grade = "C (ë³´í†µ)"
        else:
            grade = "D (ê°œì„  í•„ìš”)"
        
        report_lines.append(f"ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        report_lines.append(f"ì£¼ìš” ì§€í‘œ: í‰ê·  {avg_time:.3f}ms, ì •í™•ì„± {accuracy:.1f}%")
        
        # ê¶Œì¥ì‚¬í•­
        if accuracy < 100.0:
            report_lines.append("âš ï¸  ê¶Œì¥ì‚¬í•­: Race Condition ë°œìƒìœ¼ë¡œ ë™ì‹œì„± ì œì–´ ê°œì„  í•„ìš”")
        if avg_time > 5.0:
            report_lines.append("âš ï¸  ê¶Œì¥ì‚¬í•­: ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ ì„±ëŠ¥ ìµœì í™” í•„ìš”")
        if increment_stats['thread_consistency']['thread_consistency_rate'] < 90:
            report_lines.append("âš ï¸  ê¶Œì¥ì‚¬í•­: ìŠ¤ë ˆë“œ ì¼ê´€ì„± ë¶€ì¡±ìœ¼ë¡œ ìŠ¤ë ˆë“œ ê´€ë¦¬ ê°œì„  í•„ìš”")
    
    return "\n".join(report_lines)

def save_comprehensive_analysis(critical_timing, critical_stats, increment_timing, increment_stats, output_name, display_name):
    """
    ì¢…í•© ë¶„ì„ ê²°ê³¼ ì €ì¥ (ì‚¬ìš©ì ì§€ì • íŒŒì¼ëª…)
    """
    # concurrency_reports í´ë” ìƒì„±
    reports_dir = 'concurrency_reports'
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
        print(f"[í´ë” ìƒì„±] '{reports_dir}' í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # íŒŒì¼ëª… ì„¤ì • (ì‚¬ìš©ì ì§€ì •)
    base_name = output_name
    
    # 1. ìƒì„¸ íƒ€ì´ë° ë°ì´í„° ì €ì¥
    if not critical_timing.empty:
        critical_file = os.path.join(reports_dir, f"{base_name}_critical_timing.csv")
        critical_timing.to_csv(critical_file, index=False, encoding='utf-8-sig')
        print(f"ì„ê³„êµ¬ì—­ íƒ€ì´ë° ì €ì¥: {critical_file}")
    
    if not increment_timing.empty:
        increment_file = os.path.join(reports_dir, f"{base_name}_increment_timing.csv")
        increment_timing.to_csv(increment_file, index=False, encoding='utf-8-sig')
        print(f"ì¦ê°€ ì‘ì—… íƒ€ì´ë° ì €ì¥: {increment_file}")
    
    # 2. ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥
    report_content = generate_comprehensive_report(critical_timing, critical_stats, increment_timing, increment_stats, display_name)
    report_file = os.path.join(reports_dir, f"{base_name}_report.txt")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    
    return report_content

def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - ë‹¨ì¼ ë™ì‹œì„± ì œì–´ ê¸°ìˆ  ë¶„ì„ (íŒŒì¼ëª…ë§Œ ì§€ì •)
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="ë™ì‹œì„± ì œì–´ ê¸°ìˆ  ì¢…í•© ë¶„ì„ê¸°")
    parser.add_argument('--output', type=str, default='concurrency_analysis',
                       help='ì¶œë ¥ íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ê¸°ë³¸: concurrency_analysis)')
    parser.add_argument('--room', type=int, help='íŠ¹ì • ë°© ë²ˆí˜¸ë§Œ ë¶„ì„ (ì˜µì…˜)')
    
    args = parser.parse_args()
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    critical_log = 'critical_section_marks.csv'
    
    if args.room:
        increment_log = f'results/concurrency_timing_room{args.room}.csv'
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ë„ í™•ì¸
        if not os.path.exists(increment_log):
            increment_log = f'concurrency_timing_room{args.room}.csv'
        display_name = f"ë™ì‹œì„± ì œì–´ (ë°© {args.room})"
    else:
        increment_log = 'results/concurrency_timing_all_rooms.csv'
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ë„ í™•ì¸
        if not os.path.exists(increment_log):
            increment_log = 'concurrency_timing_all_rooms.csv'
        display_name = "ë™ì‹œì„± ì œì–´ (ì „ì²´ ë°©)"
    
    print("ğŸ”§ ë™ì‹œì„± ì œì–´ ê¸°ìˆ  ì¢…í•© ë¶„ì„ ì‹œì‘...")
    print("ğŸ“Š ë¶„ì„ ëŒ€ìƒ: WAITING_START, CRITICAL_ENTER, CRITICAL_LEAVE, INCREMENT_BEFORE, INCREMENT_AFTER")
    print(f"ğŸ“„ ì„ê³„êµ¬ì—­ ë¡œê·¸: {critical_log}")
    print(f"ğŸ“„ ì¦ê°€ ì‘ì—… ë¡œê·¸: {increment_log}")
    print(f"ğŸ“„ ì¶œë ¥ íŒŒì¼ëª…: {args.output}")
    print("")
    
    print(f"{'='*60}")
    print(f"ë¶„ì„ ì¤‘: {display_name}")
    print(f"{'='*60}")
    
    # 1. ì„ê³„êµ¬ì—­ ë¡œê·¸ ë¶„ì„ (WAITING_START, CRITICAL_ENTER, CRITICAL_LEAVE)
    print("1. ì„ê³„êµ¬ì—­ ë¶„ì„ ì¤‘...")
    df_critical = read_critical_section_logs(critical_log)
    critical_timing, critical_stats = analyze_critical_section_timing(df_critical)
    
    # 2. ì¦ê°€ ì‘ì—… ë¡œê·¸ ë¶„ì„ (INCREMENT_BEFORE, INCREMENT_AFTER)
    print("2. ì¦ê°€ ì‘ì—… ë¶„ì„ ì¤‘...")
    df_increment = read_increment_logs(increment_log)
    increment_timing, increment_stats = analyze_increment_timing(df_increment)
    
    # 3. ì¢…í•© ë¶„ì„ ê²°ê³¼ ì €ì¥
    print("3. ì¢…í•© ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
    report_content = save_comprehensive_analysis(critical_timing, critical_stats, increment_timing, increment_stats, args.output, display_name)
    
    # 4. ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ“Š {display_name} ìš”ì•½:")
    if increment_stats:
        print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {increment_stats['duration_stats_ms']['mean']:.3f} ms")
        print(f"  ì •í™•ì„±: {increment_stats['accuracy_stats']['accuracy_rate']:.1f}%")
        print(f"  ì´ ì‘ì—… ìˆ˜: {increment_stats['total_operations']}")
    if critical_stats:
        print(f"  í‰ê·  ëŒ€ê¸° ì‹œê°„: {critical_stats.get('waiting_stats', {}).get('mean', 0):.3f} ms")
        print(f"  í‰ê·  ì„ê³„êµ¬ì—­ ì‹œê°„: {critical_stats.get('critical_stats', {}).get('mean', 0):.3f} ms")
    
    print(f"\nâœ… {display_name} ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ê°€ 'concurrency_reports' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()