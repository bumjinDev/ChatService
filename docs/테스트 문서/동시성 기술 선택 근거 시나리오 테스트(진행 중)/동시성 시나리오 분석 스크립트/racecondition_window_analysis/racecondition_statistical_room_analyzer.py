#!/usr/bin/env python3
"""
Race Condition ë°©ë³„ í†µê³„ ë¶„ì„ê¸°
- ì´ìƒí˜„ìƒ íƒì§€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 4ê°€ì§€ ê·œì¹™ë³„ ë°© ë‹¨ìœ„ í†µê³„ì  ì§‘ê³„ ë¶„ì„
- ë°©ë³„ ë°œìƒë¥ , ì‹¬ê°ë„, ë¶„í¬ ë¶„ì„ ì œê³µ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import argparse
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, PatternFill, Alignment
import traceback

def load_and_validate_data(preprocessor_file, analysis_file):
    """ë°ì´í„° ë¡œë“œ ë° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦"""
    print("ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    # ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ
    preprocessor_df = pd.read_csv(preprocessor_file)
    print(f"âœ… ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(preprocessor_df)}í–‰")
    
    # ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„° ë¡œë“œ
    analysis_df = pd.read_csv(analysis_file)
    print(f"âœ… ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(analysis_df)}í–‰")
    
    # ì „ì²˜ë¦¬ ë°ì´í„° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    preprocessor_required = ['roomNumber', 'bin', 'user_id']
    missing_preprocessor = [col for col in preprocessor_required if col not in preprocessor_df.columns]
    if missing_preprocessor:
        raise ValueError(f"ì „ì²˜ë¦¬ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_preprocessor}")
    
    # ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    analysis_required = ['roomNumber', 'bin', 'anomaly_type', 'lost_update_diff', 
                        'contention_group_size', 'over_capacity_amount', 'curr_sequence_diff']
    missing_analysis = [col for col in analysis_required if col not in analysis_df.columns]
    if missing_analysis:
        raise ValueError(f"ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_analysis}")
    
    # ë°ì´í„° íƒ€ì… ì •ë¦¬
    preprocessor_df['roomNumber'] = preprocessor_df['roomNumber'].astype(int)
    preprocessor_df['bin'] = preprocessor_df['bin'].astype(int)
    
    analysis_df['roomNumber'] = analysis_df['roomNumber'].astype(int)
    analysis_df['bin'] = analysis_df['bin'].astype(int)
    
    print("âœ… ë°ì´í„° ê²€ì¦ ë° íƒ€ì… ë³€í™˜ ì™„ë£Œ")
    return preprocessor_df, analysis_df

def calculate_total_requests_per_room(preprocessor_df):
    """ë°©ë³„ ì „ì²´ ìš”ì²­ìˆ˜ ì§‘ê³„ (ëª¨ë“  bin í†µí•©)"""
    print("ğŸ“Š ë°©ë³„ ì „ì²´ ìš”ì²­ìˆ˜ ì§‘ê³„ ì¤‘...")
    
    total_requests = preprocessor_df.groupby(['roomNumber']).size().reset_index(name='total_requests')
    
    print(f"âœ… ì§‘ê³„ ì™„ë£Œ: {len(total_requests)}ê°œ ë°©")
    return total_requests

def calculate_statistics(filtered_df, value_column, total_requests_df):
    """ë°©ë³„ í†µê³„ ê³„ì‚° í•¨ìˆ˜ - ëª¨ë“  ë°© í¬í•¨"""
    # ì „ì²´ ë°© ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
    result_stats = total_requests_df.copy()
    
    # í†µê³„ ì»¬ëŸ¼ë“¤ ì´ˆê¸°í™” (ì´ìƒí˜„ìƒ ì—†ëŠ” ë°©ì€ 0 ë˜ëŠ” NaN)
    result_stats['occurrence_count'] = 0
    result_stats['occurrence_rate'] = 0.0
    result_stats['sum_value'] = 0.0
    result_stats['avg_value'] = np.nan
    result_stats['min_value'] = np.nan
    result_stats['max_value'] = np.nan
    result_stats['median_value'] = np.nan
    result_stats['std_value'] = 0.0
    
    # ì´ìƒí˜„ìƒì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤ì œ í†µê³„ ê³„ì‚°
    if len(filtered_df) > 0:
        # roomNumber ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        grouped = filtered_df.groupby(['roomNumber'])
        
        for room_num, group in grouped:
            # í•´ë‹¹ ë°©ì˜ ëª¨ë“  ê°’ë“¤ ì¶”ì¶œ (NaN ì œê±°)
            values = group[value_column].dropna()
            
            if len(values) == 0:
                continue
            
            # ê²°ê³¼ DataFrameì—ì„œ í•´ë‹¹ ë°© ì°¾ê¸°
            mask = (result_stats['roomNumber'] == room_num)
            row_idx = result_stats[mask].index
            
            if len(row_idx) == 0:
                continue
                
            row_idx = row_idx[0]
            total_requests = result_stats.loc[row_idx, 'total_requests']
            
            # í†µê³„ ê³„ì‚°
            occurrence_count = len(values)
            occurrence_rate = (occurrence_count / total_requests * 100) if total_requests > 0 else 0
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            result_stats.loc[row_idx, 'occurrence_count'] = int(occurrence_count)
            result_stats.loc[row_idx, 'occurrence_rate'] = round(occurrence_rate, 2)
            result_stats.loc[row_idx, 'sum_value'] = round(values.sum(), 2)
            result_stats.loc[row_idx, 'avg_value'] = round(values.mean(), 2)
            result_stats.loc[row_idx, 'min_value'] = round(values.min(), 2)
            result_stats.loc[row_idx, 'max_value'] = round(values.max(), 2)
            result_stats.loc[row_idx, 'median_value'] = round(values.median(), 2)
            result_stats.loc[row_idx, 'std_value'] = round(values.std(), 4) if len(values) > 1 else 0.0
    
    # ë°ì´í„° íƒ€ì… ì •ë¦¬
    result_stats['roomNumber'] = result_stats['roomNumber'].astype(int)
    result_stats['total_requests'] = result_stats['total_requests'].astype(int)
    result_stats['occurrence_count'] = result_stats['occurrence_count'].astype(int)
    
    return result_stats

def analyze_lost_update(analysis_df, total_requests_df):
    """ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ ë°©ë³„ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update) ë°©ë³„ ë¶„ì„ ì¤‘...")
    
    # 'ê°’ ë¶ˆì¼ì¹˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ê°’ ë¶ˆì¼ì¹˜', na=False)]
    print(f"  - ê°’ ë¶ˆì¼ì¹˜ ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # lost_update_diff ê¸°ì¤€ í†µê³„ ê³„ì‚°
    stats_df = calculate_statistics(filtered_df, 'lost_update_diff', total_requests_df)
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    column_mapping = {
        'sum_value': 'ì˜¤ì°¨ ëˆ„ì  ì´í•©',
        'avg_value': 'í‰ê·  ì˜¤ì°¨',
        'min_value': 'ìµœì†Œ ì˜¤ì°¨',
        'max_value': 'ìµœëŒ€ ì˜¤ì°¨',
        'median_value': 'ì¤‘ê°„ê°’ ì˜¤ì°¨',
        'std_value': 'ì˜¤ì°¨ í‘œì¤€í¸ì°¨'
    }
    
    final_columns = {
        'roomNumber': 'ë°© ë²ˆí˜¸',
        'total_requests': 'ì „ì²´ ìš”ì²­ìˆ˜',
        'occurrence_count': 'ë°œìƒ ê±´ìˆ˜',
        'occurrence_rate': 'ë°œìƒë¥  (%)',
        **column_mapping
    }
    
    stats_df = stats_df.rename(columns=final_columns)
    
    print(f"âœ… ê°’ ë¶ˆì¼ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(stats_df)}ê°œ ë°©")
    return stats_df

def analyze_contention(analysis_df, total_requests_df):
    """ê·œì¹™ 2: ê²½í•© ë°œìƒ ë°©ë³„ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 2: ê²½í•© ë°œìƒ (Contention) ë°©ë³„ ë¶„ì„ ì¤‘...")
    
    # 'ê²½í•© ë°œìƒ ì˜¤ë¥˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ê²½í•© ë°œìƒ ì˜¤ë¥˜', na=False)]
    print(f"  - ê²½í•© ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # contention_group_size ê¸°ì¤€ í†µê³„ ê³„ì‚°
    stats_df = calculate_statistics(filtered_df, 'contention_group_size', total_requests_df)
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    column_mapping = {
        'sum_value': 'ì´ ê²½í•© ìŠ¤ë ˆë“œ ìˆ˜',
        'avg_value': 'í‰ê·  ê²½í•© ìŠ¤ë ˆë“œ ìˆ˜',
        'min_value': 'ìµœì†Œ ê²½í•© ê·¸ë£¹ í¬ê¸°',
        'max_value': 'ìµœëŒ€ ê²½í•© ìŠ¤ë ˆë“œ ìˆ˜',
        'median_value': 'ì¤‘ê°„ê°’ ê²½í•© ê·¸ë£¹ í¬ê¸°',
        'std_value': 'ê²½í•© ê°•ë„ í‘œì¤€í¸ì°¨'
    }
    
    final_columns = {
        'roomNumber': 'ë°© ë²ˆí˜¸',
        'total_requests': 'ì „ì²´ ìš”ì²­ìˆ˜',
        'occurrence_count': 'ë°œìƒ ê±´ìˆ˜',
        'occurrence_rate': 'ë°œìƒë¥  (%)',
        **column_mapping
    }
    
    stats_df = stats_df.rename(columns=final_columns)
    
    print(f"âœ… ê²½í•© ë°œìƒ ë¶„ì„ ì™„ë£Œ: {len(stats_df)}ê°œ ë°©")
    return stats_df

def analyze_capacity_exceeded(analysis_df, total_requests_df):
    """ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ë°©ë³„ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 3: ì •ì› ì´ˆê³¼ (Capacity Exceeded) ë°©ë³„ ë¶„ì„ ì¤‘...")
    
    # 'ì •ì› ì´ˆê³¼ ì˜¤ë¥˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ì •ì› ì´ˆê³¼ ì˜¤ë¥˜', na=False)]
    print(f"  - ì •ì› ì´ˆê³¼ ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # over_capacity_amount ê¸°ì¤€ í†µê³„ ê³„ì‚°
    stats_df = calculate_statistics(filtered_df, 'over_capacity_amount', total_requests_df)
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    column_mapping = {
        'sum_value': 'ì´ ì´ˆê³¼ ì¸ì›',
        'avg_value': 'í‰ê·  ì´ˆê³¼ ì¸ì›',
        'min_value': 'ìµœì†Œ ì´ˆê³¼ ì¸ì›',
        'max_value': 'ìµœëŒ€ ì´ˆê³¼ ì¸ì›',
        'median_value': 'ì¤‘ê°„ê°’ ì´ˆê³¼ ì¸ì›',
        'std_value': 'ì´ˆê³¼ ê·œëª¨ í‘œì¤€í¸ì°¨'
    }
    
    final_columns = {
        'roomNumber': 'ë°© ë²ˆí˜¸',
        'total_requests': 'ì „ì²´ ìš”ì²­ìˆ˜',
        'occurrence_count': 'ë°œìƒ ê±´ìˆ˜',
        'occurrence_rate': 'ë°œìƒë¥  (%)',
        **column_mapping
    }
    
    stats_df = stats_df.rename(columns=final_columns)
    
    print(f"âœ… ì •ì› ì´ˆê³¼ ë¶„ì„ ì™„ë£Œ: {len(stats_df)}ê°œ ë°©")
    return stats_df

def analyze_state_transition(analysis_df, total_requests_df):
    """ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ë°©ë³„ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (State Transition) ë°©ë³„ ë¶„ì„ ì¤‘...")
    
    # 'ìƒíƒœ ì „ì´ ì˜¤ë¥˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ìƒíƒœ ì „ì´ ì˜¤ë¥˜', na=False)]
    print(f"  - ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # curr_sequence_diff ê¸°ì¤€ í†µê³„ ê³„ì‚°
    stats_df = calculate_statistics(filtered_df, 'curr_sequence_diff', total_requests_df)
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    column_mapping = {
        'sum_value': 'ì´ ìˆœì„œ ì°¨ì´',
        'avg_value': 'í‰ê·  ìˆœì„œ ì°¨ì´',
        'min_value': 'ìµœì†Œ ìˆœì„œ ì°¨ì´',
        'max_value': 'ìµœëŒ€ ìˆœì„œ ì°¨ì´',
        'median_value': 'ì¤‘ê°„ê°’ ìˆœì„œ ì°¨ì´',
        'std_value': 'ìˆœì„œ ì°¨ì´ í‘œì¤€í¸ì°¨'
    }
    
    final_columns = {
        'roomNumber': 'ë°© ë²ˆí˜¸',
        'total_requests': 'ì „ì²´ ìš”ì²­ìˆ˜',
        'occurrence_count': 'ë°œìƒ ê±´ìˆ˜',
        'occurrence_rate': 'ë°œìƒë¥  (%)',
        **column_mapping
    }
    
    stats_df = stats_df.rename(columns=final_columns)
    
    print(f"âœ… ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ë¶„ì„ ì™„ë£Œ: {len(stats_df)}ê°œ ë°©")
    return stats_df

def add_dataframe_to_sheet(ws, df, sheet_title):
    """DataFrameì„ ì›Œí¬ì‹œíŠ¸ì— ì¶”ê°€í•˜ê³  ìŠ¤íƒ€ì¼ ì ìš©"""
    # ì‹œíŠ¸ ì œëª© ì¶”ê°€
    ws['A1'] = sheet_title
    ws['A1'].font = Font(size=14, bold=True)
    ws['A1'].fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    ws['A1'].font = Font(size=14, bold=True, color="FFFFFF")
    
    # ë¹ˆ í–‰ ì¶”ê°€
    start_row = 3
    
    # ì»¬ëŸ¼ í—¤ë” ì¶”ê°€
    for col_idx, column in enumerate(df.columns, 1):
        cell = ws.cell(row=start_row, column=col_idx, value=column)
        cell.font = Font(bold=True)
        cell.fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
        cell.alignment = Alignment(horizontal='center')
    
    # ë°ì´í„° ì¶”ê°€
    for row_idx, row in enumerate(df.itertuples(index=False), start_row + 1):
        for col_idx, value in enumerate(row, 1):
            ws.cell(row=row_idx, column=col_idx, value=value)
    
    # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •
    for col_idx, column in enumerate(df.columns, 1):
        max_length = max(len(str(column)), 15)
        if len(df) > 0:
            max_length = max(max_length, max(len(str(val)) for val in df.iloc[:, col_idx-1] if pd.notna(val)))
        ws.column_dimensions[get_column_letter(col_idx)].width = min(max_length + 2, 30)

def create_excel_output(lost_update_df, contention_df, capacity_df, state_transition_df, output_file):
    """4ê°œ ì‹œíŠ¸ë¡œ êµ¬ì„±ëœ Excel íŒŒì¼ ìƒì„±"""
    print("ğŸ“Š Excel íŒŒì¼ ìƒì„± ì¤‘...")
    
    wb = Workbook()
    
    # ê¸°ë³¸ ì‹œíŠ¸ ì œê±°
    wb.remove(wb.active)
    
    # ì‹œíŠ¸ 1: ë°©ë³„ Lost Update ë¶„ì„
    ws1 = wb.create_sheet("Room_LostUpdate_Analysis")
    add_dataframe_to_sheet(ws1, lost_update_df, "ë°©ë³„ ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update) í†µí•© ë¶„ì„")
    
    # ì‹œíŠ¸ 2: ë°©ë³„ Contention ë¶„ì„
    ws2 = wb.create_sheet("Room_Contention_Analysis")
    add_dataframe_to_sheet(ws2, contention_df, "ë°©ë³„ ê·œì¹™ 2: ê²½í•© ë°œìƒ (Contention) í†µí•© ë¶„ì„")
    
    # ì‹œíŠ¸ 3: ë°©ë³„ Capacity Exceeded ë¶„ì„
    ws3 = wb.create_sheet("Room_Capacity_Analysis")
    add_dataframe_to_sheet(ws3, capacity_df, "ë°©ë³„ ê·œì¹™ 3: ì •ì› ì´ˆê³¼ (Capacity Exceeded) í†µí•© ë¶„ì„")
    
    # ì‹œíŠ¸ 4: ë°©ë³„ State Transition ë¶„ì„
    ws4 = wb.create_sheet("Room_StateTransition_Analysis")
    add_dataframe_to_sheet(ws4, state_transition_df, "ë°©ë³„ ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (State Transition) í†µí•© ë¶„ì„")
    
    wb.save(output_file)
    print(f"âœ… Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")

def print_summary_statistics(lost_update_df, contention_df, capacity_df, state_transition_df, total_requests_df):
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ í†µê³„ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“ˆ RACE CONDITION ë°©ë³„ í†µê³„ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    total_rooms = len(total_requests_df)
    total_requests_sum = total_requests_df['total_requests'].sum()
    
    print(f"ì „ì²´ ë¶„ì„ ëŒ€ìƒ: {total_rooms}ê°œ ë°©")
    print(f"ì „ì²´ ìš”ì²­ ìˆ˜: {total_requests_sum:,}ê±´")
    
    print(f"\n--- ê·œì¹™ë³„ ë¶„ì„ ê²°ê³¼ ---")
    
    # ê·œì¹™ 1
    lost_rooms = len(lost_update_df)
    lost_requests = lost_update_df['ë°œìƒ ê±´ìˆ˜'].sum() if len(lost_update_df) > 0 else 0
    print(f"ê·œì¹™ 1 (ê°’ ë¶ˆì¼ì¹˜): {lost_rooms}ê°œ ë°©ì—ì„œ {lost_requests}ê±´ ë°œìƒ")
    
    # ê·œì¹™ 2  
    contention_rooms = len(contention_df)
    contention_requests = contention_df['ë°œìƒ ê±´ìˆ˜'].sum() if len(contention_df) > 0 else 0
    print(f"ê·œì¹™ 2 (ê²½í•© ë°œìƒ): {contention_rooms}ê°œ ë°©ì—ì„œ {contention_requests}ê±´ ë°œìƒ")
    
    # ê·œì¹™ 3
    capacity_rooms = len(capacity_df)
    capacity_requests = capacity_df['ë°œìƒ ê±´ìˆ˜'].sum() if len(capacity_df) > 0 else 0
    print(f"ê·œì¹™ 3 (ì •ì› ì´ˆê³¼): {capacity_rooms}ê°œ ë°©ì—ì„œ {capacity_requests}ê±´ ë°œìƒ")
    
    # ê·œì¹™ 4
    state_rooms = len(state_transition_df)
    state_requests = state_transition_df['ë°œìƒ ê±´ìˆ˜'].sum() if len(state_transition_df) > 0 else 0
    print(f"ê·œì¹™ 4 (ìƒíƒœ ì „ì´): {state_rooms}ê°œ ë°©ì—ì„œ {state_requests}ê±´ ë°œìƒ")
    
    total_anomaly_requests = lost_requests + contention_requests + capacity_requests + state_requests
    anomaly_rate = (total_anomaly_requests / total_requests_sum * 100) if total_requests_sum > 0 else 0
    
    print(f"\nì „ì²´ ì´ìƒí˜„ìƒ ë°œìƒë¥ : {anomaly_rate:.2f}% ({total_anomaly_requests:,}/{total_requests_sum:,})")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Race Condition ë°©ë³„ í†µê³„ ë¶„ì„ê¸°")
    parser.add_argument('preprocessor_csv', help='ì „ì²˜ë¦¬ ê²°ê³¼ CSV íŒŒì¼')
    parser.add_argument('analysis_csv', help='ì´ìƒí˜„ìƒ ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼')
    parser.add_argument('output_xlsx', help='ë°©ë³„ í†µê³„ ë¶„ì„ Excel ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--rooms', help='ë¶„ì„í•  ë°© ë²ˆí˜¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ Race Condition ë°©ë³„ í†µê³„ ë¶„ì„ê¸° ì‹œì‘...")
        print(f"ì…ë ¥ íŒŒì¼ 1: {args.preprocessor_csv}")
        print(f"ì…ë ¥ íŒŒì¼ 2: {args.analysis_csv}")
        print(f"ì¶œë ¥ íŒŒì¼: {args.output_xlsx}")
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        preprocessor_df, analysis_df = load_and_validate_data(args.preprocessor_csv, args.analysis_csv)
        
        # 2. ë°© ë²ˆí˜¸ í•„í„°ë§ (ì„ íƒì‚¬í•­)
        if args.rooms:
            room_numbers = [int(room.strip()) for room in args.rooms.split(',')]
            preprocessor_df = preprocessor_df[preprocessor_df['roomNumber'].isin(room_numbers)]
            analysis_df = analysis_df[analysis_df['roomNumber'].isin(room_numbers)]
            print(f"ğŸ” ë°© ë²ˆí˜¸ {room_numbers}ë¡œ í•„í„°ë§ ì ìš©")
        
        # 3. ë°©ë³„ ì „ì²´ ìš”ì²­ìˆ˜ ì§‘ê³„
        total_requests_df = calculate_total_requests_per_room(preprocessor_df)
        
        # 4. 4ê°€ì§€ ê·œì¹™ë³„ í†µê³„ ë¶„ì„
        lost_update_df = analyze_lost_update(analysis_df, total_requests_df)
        contention_df = analyze_contention(analysis_df, total_requests_df)
        capacity_df = analyze_capacity_exceeded(analysis_df, total_requests_df)
        state_transition_df = analyze_state_transition(analysis_df, total_requests_df)
        
        # 5. Excel íŒŒì¼ ìƒì„±
        create_excel_output(lost_update_df, contention_df, capacity_df, state_transition_df, args.output_xlsx)
        
        # 6. ìš”ì•½ í†µê³„ ì¶œë ¥
        print_summary_statistics(lost_update_df, contention_df, capacity_df, state_transition_df, total_requests_df)
        
        print("\nğŸ‰ ë°©ë³„ í†µê³„ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    main()