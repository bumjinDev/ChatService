#!/usr/bin/env python3
"""
Race Condition ë¶„ì„ê¸° (ì™„ì „ ìˆ˜ì • ë²„ì „)
- ë””ë²„ê¹…ëœ CSV êµ¬ì¡°ì— ë§ì¶° ì™„ì „íˆ ìƒˆë¡œ ì‘ì„±
- 4ê°€ì§€ ê·œì¹™ìœ¼ë¡œ ë™ì‹œì„± ë¬¸ì œë¥¼ íƒì§€
"""

import pandas as pd
import numpy as np
from datetime import datetime
import argparse
from openpyxl import load_workbook

def detect_race_condition_anomalies(df):
    """
    4ê°€ì§€ ê·œì¹™ìœ¼ë¡œ ì´ìƒ í˜„ìƒì„ íƒì§€
    
    ì…ë ¥: df (DataFrame) - ë¶„ì„í•  ë°ì´í„°
    ì¶œë ¥: (anomalies, detailed_analysis) - ì´ìƒ í˜„ìƒ ëª©ë¡ê³¼ ìƒì„¸ ë¶„ì„
    """
    
    print("ğŸ” ì´ìƒ í˜„ìƒ íƒì§€ ì‹œì‘...")
    
    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    anomalies = []
    detailed_analysis = []
    
    # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
    df['prev_entry_time'] = pd.to_datetime(df['prev_entry_time'])
    df['curr_entry_time'] = pd.to_datetime(df['curr_entry_time'])
    
    print(f"ë¶„ì„ ëŒ€ìƒ ë°©: {df['roomNumber'].unique()}")
    
    # ë°©ë³„ë¡œ ë¶„ì„
    for room_num in df['roomNumber'].unique():
        print(f"  ë°© {room_num} ë¶„ì„ ì¤‘...")
        room_df = df[df['roomNumber'] == room_num].copy()
        
        # === ê·œì¹™ 4ë¥¼ ìœ„í•œ ì‹œê°„ ìˆœì„œ ë§¤í•‘ ===
        room_df_sorted = room_df.sort_values('curr_entry_time').reset_index(drop=True)
        sorted_sequence_map = {}
        for sorted_idx, row in room_df_sorted.iterrows():
            user_id = row['user_id']
            sorted_position = sorted_idx + 1
            sorted_sequence_map[user_id] = sorted_position
        
        # === ê·œì¹™ 2ë¥¼ ìœ„í•œ ê²½í•© ê·¸ë£¹ ì°¾ê¸° ===
        contention_groups = find_contention_groups(room_df)
        
        # === ê° ë ˆì½”ë“œ ê²€ì‚¬ ===
        for idx in room_df.index:
            row = room_df.loc[idx]
            anomaly_types = []
            anomaly_details = {}
            
            # ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜
            if pd.notna(row['expected_people']):
                expected_curr = min(row['expected_people'], row['max_people'])
                if row['curr_people'] != expected_curr:
                    anomaly_types.append('ê°’ ë¶ˆì¼ì¹˜')
                    anomaly_details['lost_update_expected'] = expected_curr
                    anomaly_details['lost_update_actual'] = row['curr_people']
                    anomaly_details['lost_update_diff'] = row['curr_people'] - expected_curr
            
            # ê·œì¹™ 2: ê²½í•© ë°œìƒ ìì²´
            user_id = row['user_id']
            if user_id in contention_groups:
                anomaly_types.append('ê²½í•© ë°œìƒ ìì²´')
                contention_info = contention_groups[user_id]
                anomaly_details['contention_group_size'] = contention_info['group_size']
                anomaly_details['contention_user_ids'] = ', '.join(contention_info['user_ids'])
            
            # ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ì˜¤ë¥˜
            if row['curr_people'] > row['max_people']:
                anomaly_types.append('ì •ì› ì´ˆê³¼ ì˜¤ë¥˜')
                anomaly_details['over_capacity_amount'] = row['curr_people'] - row['max_people']
                anomaly_details['over_capacity_curr'] = row['curr_people']
                anomaly_details['over_capacity_max'] = row['max_people']
            
            # ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜
            if user_id in sorted_sequence_map:
                sorted_position = sorted_sequence_map[user_id]
                expected_curr_people = 1 + sorted_position  # ì´ˆê¸° 1ëª… + ìˆœë²ˆ
                
                if expected_curr_people <= row['max_people']:
                    if row['curr_people'] != expected_curr_people:
                        anomaly_types.append('ìƒíƒœ ì „ì´ ì˜¤ë¥˜')
                        anomaly_details['expected_curr_by_sequence'] = expected_curr_people
                        anomaly_details['actual_curr_people'] = row['curr_people']
                        anomaly_details['curr_sequence_diff'] = row['curr_people'] - expected_curr_people
                        anomaly_details['sorted_sequence_position'] = sorted_position
            
            # ì„ê³„êµ¬ì—­ ë¶„ì„
            critical_analysis = analyze_critical_section(row, room_df, idx)
            anomaly_details.update(critical_analysis)
            
            # ì´ìƒ í˜„ìƒ ë°œê²¬ ì‹œ ì €ì¥
            if anomaly_types:
                result_row = row.to_dict()
                result_row['anomaly_type'] = ', '.join(anomaly_types)
                
                for key, value in anomaly_details.items():
                    result_row[key] = value
                
                anomalies.append(result_row)
                
                # ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±
                detailed_text = generate_analysis_text(row, anomaly_types, anomaly_details, room_num)
                detailed_analysis.append(detailed_text)
    
    print(f"âœ… ì´ìƒ í˜„ìƒ íƒì§€ ì™„ë£Œ: {len(anomalies)}ê±´ ë°œê²¬")
    return anomalies, detailed_analysis

def find_contention_groups(room_df):
    """ì§„ì§œ ì„ê³„êµ¬ì—­ ê¸°ì¤€ ê²½í•© ê·¸ë£¹ ì°¾ê¸°"""
    contention_groups = {}
    
    for i, row1 in room_df.iterrows():
        start1 = row1['prev_entry_time']
        end1 = row1['curr_entry_time']
        user1 = row1['user_id']
        
        if pd.isna(start1) or pd.isna(end1):
            continue
            
        overlapping_users = [user1]
        
        for j, row2 in room_df.iterrows():
            if i == j:
                continue
                
            start2 = row2['prev_entry_time']
            end2 = row2['curr_entry_time']
            user2 = row2['user_id']
            
            if pd.isna(start2) or pd.isna(end2):
                continue
            
            # ì‹œê°„ ê²¹ì¹¨ í™•ì¸
            if not (end1 < start2 or end2 < start1):
                overlapping_users.append(user2)
        
        # 2ëª… ì´ìƒ ê²¹ì¹˜ë©´ ê²½í•©
        if len(overlapping_users) >= 2:
            for user_id in overlapping_users:
                contention_groups[user_id] = {
                    'group_size': len(overlapping_users),
                    'user_ids': overlapping_users
                }
    
    return contention_groups

def analyze_critical_section(base_row, room_df, base_idx):
    """ì„ê³„êµ¬ì—­ ìƒì„¸ ë¶„ì„"""
    critical_start = base_row['prev_entry_time']
    critical_end = base_row['curr_entry_time']
    
    if pd.isna(critical_start) or pd.isna(critical_end):
        return {}
    
    # ê°œì… ì‚¬ìš©ì ì°¾ê¸°
    intervening_users = []
    for idx, other_row in room_df.iterrows():
        if idx == base_idx:
            continue
            
        other_start = other_row['prev_entry_time']
        other_end = other_row['curr_entry_time']
        
        if pd.isna(other_start) or pd.isna(other_end):
            continue
        
        if not (critical_end < other_start or other_end < critical_start):
            intervening_users.append(other_row['user_id'])
    
    result = {
        'true_critical_section_start': critical_start,
        'true_critical_section_end': critical_end,
        'true_critical_section_duration': (critical_end - critical_start).total_seconds(),
        'intervening_users_in_critical_section': ', '.join(intervening_users) if intervening_users else '',
        'intervening_user_count_critical': len(intervening_users)
    }
    
    # ë‚˜ë…¸ì´ˆ ì •ë°€ë„ ë¶„ì„
    start_nano = base_row.get('true_critical_section_nanoTime_start')
    end_nano = base_row.get('true_critical_section_nanoTime_end')
    
    if not pd.isna(start_nano) and not pd.isna(end_nano):
        result['true_critical_section_duration_nanos'] = end_nano - start_nano
        
        start_epoch = base_row.get('true_critical_section_epochNano_start')
        end_epoch = base_row.get('true_critical_section_epochNano_end')
        
        if not pd.isna(start_epoch) and not pd.isna(end_epoch):
            result['true_critical_section_epoch_duration_nanos'] = end_epoch - start_epoch
    
    return result

def generate_analysis_text(row, anomaly_types, anomaly_details, room_num):
    """ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±"""
    text = f"""
================================================================================
ê²½ìŸ ìƒíƒœ ì´ìƒ í˜„ìƒ ë¶„ì„ (ìˆ˜ì •ëœ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜)
================================================================================
ë°© ë²ˆí˜¸: {room_num}
ì‚¬ìš©ì ID: {row['user_id']}
Bin: {row['bin']}
ë°œìƒ ì‹œê°: {row['curr_entry_time']}
ì´ìƒ í˜„ìƒ ìœ í˜•: {', '.join(anomaly_types)}
ì›ë³¸ ë°© ì…ì¥ ìˆœë²ˆ: {row['room_entry_sequence']}

ê¸°ë³¸ ì •ë³´:
 ì´ì „ ì¸ì› (prev_people): {row['prev_people']}ëª…
 í˜„ì¬ ì¸ì› (curr_people): {row['curr_people']}ëª…
 ìµœëŒ€ ì •ì› (max_people): {row['max_people']}ëª…

ì§„ì§œ ì„ê³„êµ¬ì—­ ì •ë³´:
 ì‹œì‘: {row['prev_entry_time']}
 ë: {row['curr_entry_time']}
 ë‚˜ë…¸ì´ˆ ì‹œì‘: {row.get('true_critical_section_nanoTime_start', 'N/A')}
 ë‚˜ë…¸ì´ˆ ë: {row.get('true_critical_section_nanoTime_end', 'N/A')}

ìƒì„¸ ë¶„ì„:"""

    for anomaly_type in anomaly_types:
        if anomaly_type == 'ê°’ ë¶ˆì¼ì¹˜':
            text += f"""
 [ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update)]
  - ì˜ˆìƒ ê²°ê³¼: {anomaly_details.get('lost_update_expected', 'N/A')}ëª…
  - ì‹¤ì œ ê²°ê³¼: {anomaly_details.get('lost_update_actual', 'N/A')}ëª…
  - ì°¨ì´: {anomaly_details.get('lost_update_diff', 'N/A')}ëª…
  â†’ ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ì‘ì—…ìœ¼ë¡œ ì¸í•´ ì˜ë„í•œ ê°±ì‹ ì´ ëˆ„ë½ë˜ê±°ë‚˜ ë®ì–´ì“°ì—¬ì§"""
        
        elif anomaly_type == 'ê²½í•© ë°œìƒ ìì²´':
            text += f"""
 [ê·œì¹™ 2: ê²½í•© ë°œìƒ ìì²´ (Contention Detected)]
  - ê²½í•© ê·¸ë£¹ í¬ê¸°: {anomaly_details.get('contention_group_size', 'N/A')}ê°œ ì‚¬ìš©ì
  - ê²½í•© ì‚¬ìš©ì ID: {anomaly_details.get('contention_user_ids', 'N/A')}
  â†’ ì§„ì§œ ì„ê³„êµ¬ì—­ì´ 1ë‚˜ë…¸ì´ˆë¼ë„ ê²¹ì³ ë™ì‹œì„± ì œì–´ ë¶€ì¬ ìƒí™©"""
        
        elif anomaly_type == 'ì •ì› ì´ˆê³¼ ì˜¤ë¥˜':
            text += f"""
 [ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ì˜¤ë¥˜ (Capacity Exceeded Error)]
  - ìµœëŒ€ ì •ì›: {anomaly_details.get('over_capacity_max', 'N/A')}ëª…
  - ì‹¤ì œ ì¸ì›: {anomaly_details.get('over_capacity_curr', 'N/A')}ëª…
  - ì´ˆê³¼ ì¸ì›: {anomaly_details.get('over_capacity_amount', 'N/A')}ëª…
  â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì„ ëª…ë°±íˆ ìœ„ë°˜í•œ ì‹¬ê°í•œ ì˜¤ë¥˜"""
        
        elif anomaly_type == 'ìƒíƒœ ì „ì´ ì˜¤ë¥˜':
            pos = anomaly_details.get('sorted_sequence_position', 'N/A')
            text += f"""
 [ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (Stale Read / Inconsistent State)]
  - curr_entry_time ê¸°ì¤€ ì •ë ¬ ìˆœë²ˆ: {pos}ë²ˆì§¸
  - ì˜ˆìƒ curr_people: 1 + {pos} = {anomaly_details.get('expected_curr_by_sequence', 'N/A')}ëª…
  - ì‹¤ì œ curr_people: {anomaly_details.get('actual_curr_people', 'N/A')}ëª…
  - ì°¨ì´: {anomaly_details.get('curr_sequence_diff', 'N/A')}ëª…
  â†’ ì˜¬ë°”ë¥¸ ìˆœì„œì˜ ìƒíƒœë¥¼ ì½ì§€ ëª»í•˜ê³  ì˜¤ì—¼ëœ ìƒíƒœ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì—…"""

    duration = anomaly_details.get('true_critical_section_duration', 'N/A')
    duration_formatted = f"{duration:.6f}" if duration != 'N/A' else 'N/A'
    
    text += f"""

íƒ€ì´ë° ìƒì„¸ ì •ë³´:
 ì§„ì§œ ì„ê³„êµ¬ì—­ ì§€ì†ì‹œê°„: {duration_formatted}ì´ˆ
 ë‚˜ë…¸ì´ˆ ì§€ì†ì‹œê°„: {anomaly_details.get('true_critical_section_duration_nanos', 'N/A')}ns
 Epoch ë‚˜ë…¸ì´ˆ ì§€ì†ì‹œê°„: {anomaly_details.get('true_critical_section_epoch_duration_nanos', 'N/A')}ns
 ê°œì… ì‚¬ìš©ì: {anomaly_details.get('intervening_user_count_critical', 'N/A')}ê°œ
 ê°œì… ì‚¬ìš©ì ëª©ë¡: {anomaly_details.get('intervening_users_in_critical_section', 'N/A') or 'ì—†ìŒ'}
"""
    
    return text

def print_statistics(df, anomaly_df):
    """ë¶„ì„ ê²°ê³¼ í†µê³„ ì¶œë ¥"""
    print("\n=== ğŸ”§ ê²½ìŸ ìƒíƒœ íƒì§€ ê²°ê³¼ ===")
    print(f"ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
    print(f"ì´ìƒ í˜„ìƒ ë°œê²¬ ìˆ˜: {len(anomaly_df)}")
    
    if len(df) > 0:
        print(f"ì´ìƒ í˜„ìƒ ë¹„ìœ¨: {len(anomaly_df)/len(df)*100:.2f}%")
    
    # ë‚˜ë…¸ì´ˆ ì •ë°€ë„ í†µê³„
    if 'true_critical_section_nanoTime_start' in df.columns:
        nano_count = df['true_critical_section_nanoTime_start'].notna().sum()
        print(f"ë‚˜ë…¸ì´ˆ ì •ë°€ë„ ë°ì´í„°: {nano_count}ê±´ ({nano_count/len(df)*100:.1f}%)")
    
    if len(anomaly_df) > 0:
        print("\n=== 4ê°€ì§€ ê·œì¹™ë³„ ì´ìƒ í˜„ìƒ ë¶„í¬ ===")
        
        error_counts = {
            'ê°’ ë¶ˆì¼ì¹˜': 0,
            'ê²½í•© ë°œìƒ ìì²´': 0,
            'ì •ì› ì´ˆê³¼ ì˜¤ë¥˜': 0,
            'ìƒíƒœ ì „ì´ ì˜¤ë¥˜': 0
        }
        
        for anomaly_str in anomaly_df['anomaly_type']:
            for error_type in error_counts.keys():
                if error_type in anomaly_str:
                    error_counts[error_type] += 1
        
        for error_type, count in error_counts.items():
            percentage = count/len(anomaly_df)*100 if len(anomaly_df) > 0 else 0
            print(f"  - {error_type}: {count}ê±´ ({percentage:.1f}%)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Race Condition ë¶„ì„ê¸° (ì™„ì „ ìˆ˜ì • ë²„ì „)")
    parser.add_argument('input_csv', help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('output_csv', help='ì¶œë ¥ CSV íŒŒì¼ (ì´ìƒ í˜„ìƒë§Œ)')
    parser.add_argument('--detailed_output', default='detailed_analysis.txt', help='ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ íŒŒì¼')
    parser.add_argument('--rooms', help='ë¶„ì„í•  ë°© ë²ˆí˜¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
    parser.add_argument('--xlsx_output', help='Excel ì¶œë ¥ íŒŒì¼ (ì„ íƒì‚¬í•­)')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ Race Condition ë¶„ì„ê¸° ì‹œì‘...")
        
        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(args.input_csv)
        print(f"âœ… CSV íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['roomNumber', 'bin', 'user_id', 'prev_people', 'curr_people', 'expected_people', 
                           'max_people', 'prev_entry_time', 'curr_entry_time', 'room_entry_sequence']
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            return
        
        print("âœ… í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ì™„ë£Œ")
        
        # ë°© ë²ˆí˜¸ í•„í„°ë§
        if args.rooms:
            room_numbers = [int(room.strip()) for room in args.rooms.split(',')]
            df = df[df['roomNumber'].isin(room_numbers)]
            print(f"ğŸ” ë°© ë²ˆí˜¸ {room_numbers}ë¡œ í•„í„°ë§: {len(df)} ë ˆì½”ë“œ")
        
        # ë©”ì¸ ë¶„ì„ ì‹¤í–‰
        anomalies, detailed_analysis = detect_race_condition_anomalies(df)
        
        # ê²°ê³¼ ì €ì¥
        if anomalies:
            anomaly_df = pd.DataFrame(anomalies)
            
            # ì»¬ëŸ¼ ì •ë¦¬
            basic_cols = ['roomNumber', 'bin', 'user_id', 'prev_people', 'curr_people', 'expected_people', 
                         'max_people', 'prev_entry_time', 'curr_entry_time', 
                         'true_critical_section_nanoTime_start', 'true_critical_section_epochNano_start',
                         'true_critical_section_nanoTime_end', 'true_critical_section_epochNano_end',
                         'anomaly_type', 'room_entry_sequence']
            
            detail_cols = ['lost_update_expected', 'lost_update_actual', 'lost_update_diff',
                          'contention_group_size', 'contention_user_ids',
                          'over_capacity_amount', 'over_capacity_curr', 'over_capacity_max',
                          'expected_curr_by_sequence', 'actual_curr_people', 'curr_sequence_diff',
                          'sorted_sequence_position',
                          'true_critical_section_start', 'true_critical_section_end', 'true_critical_section_duration',
                          'intervening_users_in_critical_section', 'intervening_user_count_critical',
                          'true_critical_section_duration_nanos', 'true_critical_section_epoch_duration_nanos']
            
            existing_cols = [col for col in basic_cols + detail_cols if col in anomaly_df.columns]
            anomaly_df = anomaly_df[existing_cols]
            anomaly_df = anomaly_df.fillna('')
            
            # CSV ì €ì¥
            anomaly_df.to_csv(args.output_csv, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ì´ìƒ í˜„ìƒ {len(anomaly_df)}ê°œê°€ {args.output_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # Excel ì €ì¥ (ì„ íƒì‚¬í•­)
            if args.xlsx_output:
                desc_table = [
                    ["ê·œì¹™", "ì¡°ê±´", "ì˜ë¯¸"],
                    ["ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜", "curr_people â‰  expected_people", "ë‹¤ë¥¸ ì‚¬ìš©ì ì‘ì—…ìœ¼ë¡œ ì˜ë„í•œ ê°±ì‹ ì´ ëˆ„ë½/ë®ì–´ì“°ì—¬ì§"],
                    ["ê·œì¹™ 2: ê²½í•© ë°œìƒ ìì²´", "ì§„ì§œ ì„ê³„êµ¬ì—­ì´ 1ë‚˜ë…¸ì´ˆë¼ë„ ê²¹ì¹¨", "ë™ì‹œì„± ì œì–´ ë¶€ì¬ë¡œ ì ì¬ì  ìœ„í—˜ ë…¸ì¶œ"],
                    ["ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ì˜¤ë¥˜", "curr_people > max_people", "ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì„ ëª…ë°±íˆ ìœ„ë°˜í•œ ì‹¬ê°í•œ ì˜¤ë¥˜"],
                    ["ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜", "curr_people â‰  1+curr_entry_timeê¸°ì¤€ìˆœë²ˆ", "ì˜¬ë°”ë¥¸ ìƒíƒœë¥¼ ì½ì§€ ëª»í•˜ê³  ì˜¤ì—¼ëœ ìƒíƒœë¡œ ì‘ì—…"]
                ]
                
                anomaly_df.to_excel(args.xlsx_output, index=False)
                wb = load_workbook(args.xlsx_output)
                ws = wb.active
                
                for i, row in enumerate(desc_table):
                    for j, val in enumerate(row):
                        ws.cell(row=4+i, column=10+j, value=val)
                
                wb.save(args.xlsx_output)
                print(f"ğŸ“Š Excel íŒŒì¼ë„ ì €ì¥ë¨: {args.xlsx_output}")
        
        else:
            anomaly_df = pd.DataFrame()
            print("âœ… ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ì €ì¥
        with open(args.detailed_output, 'w', encoding='utf-8') as f:
            f.write("Race Condition ìƒì„¸ ë¶„ì„ ê²°ê³¼\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì…ë ¥ íŒŒì¼: {args.input_csv}\n")
            f.write(f"ì´ ì´ìƒ í˜„ìƒ ìˆ˜: {len(anomalies)}\n\n")
            
            for detailed_text in detailed_analysis:
                f.write(detailed_text)
                f.write("\n")
        
        print(f"ğŸ“„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {args.detailed_output}")
        
        # í†µê³„ ì¶œë ¥
        print_statistics(df, anomaly_df)
        
        print("\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()