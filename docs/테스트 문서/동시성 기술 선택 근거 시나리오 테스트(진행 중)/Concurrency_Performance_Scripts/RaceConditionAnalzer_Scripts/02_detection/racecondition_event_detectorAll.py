#!/usr/bin/env python3
"""
Race Condition ë¶„ì„ê¸° (ìˆ˜ì •ëœ ë²„ì „)
- ì´ìƒí˜„ìƒì´ ì—†ì–´ë„ ì „ì²´ 32ê°œ ì»¬ëŸ¼ì„ ëª¨ë‘ ì ì ˆí•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›Œì„œ ì—‘ì…€ íŒŒì¼ ìƒì„±
- ëª¨ë“  ë ˆì½”ë“œì— ëŒ€í•´ ì„ê³„êµ¬ì—­ ë¶„ì„ ìˆ˜í–‰
- ê²½í•©ì´ ì—†ëŠ” ì‚¬ìš©ìë„ ê¸°ë³¸ê°’ ì„¤ì •
"""

import pandas as pd
import numpy as np
from datetime import datetime
import argparse
from openpyxl import load_workbook

def detect_race_condition_anomalies(df):
    """
    4ê°€ì§€ ê·œì¹™ìœ¼ë¡œ ì´ìƒ í˜„ìƒì„ íƒì§€í•˜ê³  ëª¨ë“  ë ˆì½”ë“œì— ëŒ€í•´ ì™„ì „í•œ ë¶„ì„ ìˆ˜í–‰
    """
    
    print("ğŸ” ì´ìƒ í˜„ìƒ íƒì§€ ì‹œì‘...")
    
    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    anomalies = []
    detailed_analysis = []
    
    # ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
    df['prev_entry_time'] = pd.to_datetime(df['prev_entry_time'])
    df['curr_entry_time'] = pd.to_datetime(df['curr_entry_time'])
    
    print(f"ë¶„ì„ ëŒ€ìƒ ë°©: {df['roomNumber'].unique()}")
    
    # ë°©ë³„ë¡œ ë¶„ì„
    for room_num in df['roomNumber'].unique():
        print(f"  ë°© {room_num} ë¶„ì„ ì¤‘...")
        room_df = df[df['roomNumber'] == room_num].copy()
        
        # === ê·œì¹™ 2ë¥¼ ìœ„í•œ ê²½í•© ê·¸ë£¹ ì°¾ê¸° ===
        contention_groups = find_contention_groups(room_df)
        
        # === ê° ë ˆì½”ë“œ ê²€ì‚¬ ===
        for idx in room_df.index:
            row = room_df.loc[idx]
            anomaly_types = []
            anomaly_details = {}
            
            # ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜
            if pd.notna(row['expected_people']):
                expected_curr = min(row['expected_people'], row['max_people'])
                if row['curr_people'] != expected_curr:
                    anomaly_types.append('ê°’ ë¶ˆì¼ì¹˜')
                    anomaly_details['lost_update_expected'] = expected_curr
                    anomaly_details['lost_update_actual'] = row['curr_people']
                    anomaly_details['lost_update_diff'] = row['curr_people'] - expected_curr
            
            # ê·œì¹™ 2: ê²½í•© ë°œìƒ ìì²´
            user_id = row['user_id']
            if user_id in contention_groups:
                anomaly_types.append('ê²½í•© ë°œìƒ ì˜¤ë¥˜')
                contention_info = contention_groups[user_id]
                anomaly_details['contention_group_size'] = contention_info['group_size']
                anomaly_details['contention_user_ids'] = ', '.join(contention_info['user_ids'])
            else:
                # ê²½í•©ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì • (ë³¸ì¸ í¬í•¨)
                anomaly_details['contention_group_size'] = 1
                anomaly_details['contention_user_ids'] = user_id
            
            # ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ì˜¤ë¥˜ (ì§„ì… ë‹¹ì‹œ ìµœëŒ€ê°’ì„ ë„˜ì§€ ì•Šì•˜ë˜ ê²½ìš°ë§Œ)
            if (row['prev_people'] <= row['max_people'] and row['curr_people'] > row['max_people']):
                anomaly_types.append('ì •ì› ì´ˆê³¼ ì˜¤ë¥˜')
                anomaly_details['over_capacity_amount'] = row['curr_people'] - row['max_people']
                anomaly_details['over_capacity_curr'] = row['curr_people']
                anomaly_details['over_capacity_max'] = row['max_people']
            
            # ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (ì›ë³¸ room_entry_sequence ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            if row['join_result'] == 'SUCCESS':
                expected_curr_people = 1 + row['room_entry_sequence']
                if row['curr_people'] != expected_curr_people:
                    anomaly_types.append('ìƒíƒœ ì „ì´ ì˜¤ë¥˜')
                    anomaly_details['expected_curr_by_sequence'] = expected_curr_people
                    anomaly_details['actual_curr_people'] = row['curr_people']
                    anomaly_details['curr_sequence_diff'] = row['curr_people'] - expected_curr_people
                    anomaly_details['sorted_sequence_position'] = row['room_entry_sequence']
            
            # ëª¨ë“  ë ˆì½”ë“œì— ëŒ€í•´ ì„ê³„êµ¬ì—­ ë¶„ì„
            critical_analysis = analyze_critical_section(row, room_df, idx)
            anomaly_details.update(critical_analysis)
            
            # ê²°ê³¼ í–‰ ìƒì„± (ì´ìƒí˜„ìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            result_row = row.to_dict()
            result_row['anomaly_type'] = ', '.join(anomaly_types) if anomaly_types else ''
            
            # ê¸°ë³¸ê°’ ì„¤ì • (ì´ìƒí˜„ìƒì´ ì—†ëŠ” ê²½ìš°)
            default_values = {
                'lost_update_expected': 0,
                'lost_update_actual': 0,
                'lost_update_diff': 0,
                'over_capacity_amount': 0,
                'over_capacity_curr': 0,
                'over_capacity_max': 0,
                'expected_curr_by_sequence': 0,
                'actual_curr_people': 0,
                'curr_sequence_diff': 0,
                'sorted_sequence_position': 0
            }
            
            # ê¸°ë³¸ê°’ ì ìš© í›„ ì‹¤ì œê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
            for key, default_val in default_values.items():
                if key not in anomaly_details:
                    result_row[key] = default_val
                else:
                    result_row[key] = anomaly_details[key]
            
            # ë‚˜ë¨¸ì§€ anomaly_details ì¶”ê°€
            for key, value in anomaly_details.items():
                if key not in default_values:
                    result_row[key] = value
            
            anomalies.append(result_row)
            
            # ì´ìƒ í˜„ìƒì´ ìˆëŠ” ê²½ìš°ë§Œ ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±
            if anomaly_types:
                detailed_text = generate_analysis_text(row, anomaly_types, anomaly_details, room_num)
                detailed_analysis.append(detailed_text)
    
    print(f"âœ… ì „ì²´ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ: {len(anomalies)}ê±´")
    
    # ì´ìƒí˜„ìƒì´ ìˆëŠ” ë ˆì½”ë“œë§Œ ì¹´ìš´íŠ¸
    actual_anomalies = [a for a in anomalies if a['anomaly_type'] != '']
    print(f"âœ… ì´ìƒ í˜„ìƒ íƒì§€ ì™„ë£Œ: {len(actual_anomalies)}ê±´ ë°œê²¬")
    
    return anomalies, detailed_analysis

def find_contention_groups(room_df):
    """ë‚˜ë…¸ì´ˆ ì •ë°€ë„ ê¸°ë°˜ ê²½í•© ê·¸ë£¹ ì°¾ê¸°"""
    contention_groups = {}
    
    for i, row1 in room_df.iterrows():
        # ë‚˜ë…¸ì´ˆ ë°ì´í„° ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
        start1_nano = row1['true_critical_section_nanoTime_start']
        end1_nano = row1['true_critical_section_nanoTime_end']
        user1 = row1['user_id']
        
        if pd.isna(start1_nano) or pd.isna(end1_nano):
            continue
            
        overlapping_users = [user1]
        
        for j, row2 in room_df.iterrows():
            if i == j:
                continue
                
            # ë‚˜ë…¸ì´ˆ ë°ì´í„° ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
            start2_nano = row2['true_critical_section_nanoTime_start']
            end2_nano = row2['true_critical_section_nanoTime_end']
            user2 = row2['user_id']
            
            if pd.isna(start2_nano) or pd.isna(end2_nano):
                continue
            
            # ë‚˜ë…¸ì´ˆ ê¸°ì¤€ ê²¹ì¹¨ í™•ì¸ìœ¼ë¡œ ë³€ê²½
            if not (end1_nano < start2_nano or end2_nano < start1_nano):
                overlapping_users.append(user2)
        
        # 2ëª… ì´ìƒ ê²¹ì¹˜ë©´ ê²½í•©
        if len(overlapping_users) >= 2:
            for user_id in overlapping_users:
                contention_groups[user_id] = {
                    'group_size': len(overlapping_users),
                    'user_ids': overlapping_users
                }
    
    return contention_groups

def analyze_critical_section(base_row, room_df, base_idx):
    """ëª¨ë“  ë ˆì½”ë“œì— ëŒ€í•´ ì„ê³„êµ¬ì—­ ë¶„ì„ ìˆ˜í–‰"""
    # ë‚˜ë…¸ì´ˆ ë°ì´í„° í™•ì¸
    start_nano = base_row.get('true_critical_section_nanoTime_start')
    end_nano = base_row.get('true_critical_section_nanoTime_end')
    
    if pd.isna(start_nano) or pd.isna(end_nano):
        return {
            'intervening_users_in_critical_section': '',
            'intervening_user_count_critical': 0,
            'true_critical_section_duration_nanos': 0
        }
    
    # ë‚˜ë…¸ì´ˆ ê¸°ì¤€ ê°œì… ì‚¬ìš©ì ì°¾ê¸°
    intervening_users = []
    for idx, other_row in room_df.iterrows():
        if idx == base_idx:
            continue
            
        other_start_nano = other_row.get('true_critical_section_nanoTime_start')
        other_end_nano = other_row.get('true_critical_section_nanoTime_end')
        
        if pd.isna(other_start_nano) or pd.isna(other_end_nano):
            continue
        
        # ë‚˜ë…¸ì´ˆ ê¸°ì¤€ ê²¹ì¹¨ í™•ì¸
        if not (end_nano < other_start_nano or other_end_nano < start_nano):
            intervening_users.append(other_row['user_id'])
    
    result = {
        'intervening_users_in_critical_section': ', '.join(intervening_users) if intervening_users else '',
        'intervening_user_count_critical': len(intervening_users),
        'true_critical_section_duration_nanos': end_nano - start_nano
    }
    
    return result

def generate_analysis_text(row, anomaly_types, anomaly_details, room_num):
    """ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±"""
    text = f"""
================================================================================
ê²½ìŸ ìƒíƒœ ì´ìƒ í˜„ìƒ ë¶„ì„ (ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©)
================================================================================
ë°© ë²ˆí˜¸: {room_num}
ì‚¬ìš©ì ID: {row['user_id']}
Bin: {row['bin']}
ë°œìƒ ì‹œê°: {row['curr_entry_time']}
ì´ìƒ í˜„ìƒ ìœ í˜•: {', '.join(anomaly_types)}
ì›ë³¸ ë°© ì…ì¥ ìˆœë²ˆ: {row['room_entry_sequence']}

ê¸°ë³¸ ì •ë³´:
 ì´ì „ ì¸ì› (prev_people): {row['prev_people']}ëª…
 í˜„ì¬ ì¸ì› (curr_people): {row['curr_people']}ëª…
 ìµœëŒ€ ì •ì› (max_people): {row['max_people']}ëª…

ì§„ì§œ ì„ê³„êµ¬ì—­ ì •ë³´:
 ì‹œì‘: {row['prev_entry_time']}
 ë: {row['curr_entry_time']}
 ë‚˜ë…¸ì´ˆ ì‹œì‘: {row.get('true_critical_section_nanoTime_start', 'N/A')}
 ë‚˜ë…¸ì´ˆ ë: {row.get('true_critical_section_nanoTime_end', 'N/A')}

ìƒì„¸ ë¶„ì„:"""

    for anomaly_type in anomaly_types:
        if anomaly_type == 'ê°’ ë¶ˆì¼ì¹˜':
            text += f"""
 [ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update)]
  - ì˜ˆìƒ ê²°ê³¼: {anomaly_details.get('lost_update_expected', 'N/A')}ëª…
  - ì‹¤ì œ ê²°ê³¼: {anomaly_details.get('lost_update_actual', 'N/A')}ëª…
  - ì°¨ì´: {anomaly_details.get('lost_update_diff', 'N/A')}ëª…
  â†’ ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ì‘ì—…ìœ¼ë¡œ ì¸í•´ ì˜ë„í•œ ê°±ì‹ ì´ ëˆ„ë½ë˜ê±°ë‚˜ ë®ì–´ì“°ì—¬ì§"""
        
        elif anomaly_type == 'ê²½í•© ë°œìƒ ì˜¤ë¥˜':
            text += f"""
 [ê·œì¹™ 2: ê²½í•© ë°œìƒ ìì²´ (Contention Detected)]
  - ê²½í•© ê·¸ë£¹ í¬ê¸°: {anomaly_details.get('contention_group_size', 'N/A')}ê°œ ì‚¬ìš©ì
  - ê²½í•© ì‚¬ìš©ì ID: {anomaly_details.get('contention_user_ids', 'N/A')}
  â†’ ì§„ì§œ ì„ê³„êµ¬ì—­ì´ 1ë‚˜ë…¸ì´ˆë¼ë„ ê²¹ì³ ë™ì‹œì„± ì œì–´ ë¶€ì¬ ìƒí™©"""
        
        elif anomaly_type == 'ì •ì› ì´ˆê³¼ ì˜¤ë¥˜':
            text += f"""
 [ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ì˜¤ë¥˜ (Capacity Exceeded Error)]
  - ìµœëŒ€ ì •ì›: {anomaly_details.get('over_capacity_max', 'N/A')}ëª…
  - ì‹¤ì œ ì¸ì›: {anomaly_details.get('over_capacity_curr', 'N/A')}ëª…
  - ì´ˆê³¼ ì¸ì›: {anomaly_details.get('over_capacity_amount', 'N/A')}ëª…
  â†’ ì§„ì… ë‹¹ì‹œ ì •ì› ë‚´ì˜€ì§€ë§Œ ì‘ì—… í›„ ì •ì› ì´ˆê³¼í•œ ì‹¬ê°í•œ ì˜¤ë¥˜"""
        
        elif anomaly_type == 'ìƒíƒœ ì „ì´ ì˜¤ë¥˜':
            pos = anomaly_details.get('sorted_sequence_position', 'N/A')
            text += f"""
 [ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (Stale Read / Inconsistent State)]
  - ì›ë³¸ room_entry_sequence: {pos}ë²ˆì§¸
  - ì˜ˆìƒ curr_people: 1 + {pos} = {anomaly_details.get('expected_curr_by_sequence', 'N/A')}ëª…
  - ì‹¤ì œ curr_people: {anomaly_details.get('actual_curr_people', 'N/A')}ëª…
  - ì°¨ì´: {anomaly_details.get('curr_sequence_diff', 'N/A')}ëª…
  â†’ ì˜¬ë°”ë¥¸ ìˆœì„œì˜ ìƒíƒœë¥¼ ì½ì§€ ëª»í•˜ê³  ì˜¤ì—¼ëœ ìƒíƒœ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì—…"""

    duration = anomaly_details.get('true_critical_section_duration', 'N/A')
    duration_formatted = f"{duration:.6f}" if duration != 'N/A' else 'N/A'
    
    text += f"""

íƒ€ì´ë° ìƒì„¸ ì •ë³´:
 ì§„ì§œ ì„ê³„êµ¬ì—­ ì§€ì†ì‹œê°„: {duration_formatted}ì´ˆ
 ë‚˜ë…¸ì´ˆ ì§€ì†ì‹œê°„: {anomaly_details.get('true_critical_section_duration_nanos', 'N/A')}ns
 ê°œì… ì‚¬ìš©ì: {anomaly_details.get('intervening_user_count_critical', 'N/A')}ê°œ
 ê°œì… ì‚¬ìš©ì ëª©ë¡: {anomaly_details.get('intervening_users_in_critical_section', 'N/A') or 'ì—†ìŒ'}
"""
    
    return text

def print_statistics(df, anomaly_df):
    """ë¶„ì„ ê²°ê³¼ í†µê³„ ì¶œë ¥"""
    print("\n=== ğŸ”§ ê²½ìŸ ìƒíƒœ íƒì§€ ê²°ê³¼ ===")
    print(f"ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
    
    # ì‹¤ì œ ì´ìƒí˜„ìƒì´ ìˆëŠ” ë ˆì½”ë“œë§Œ ì¹´ìš´íŠ¸
    actual_anomalies = anomaly_df[anomaly_df['anomaly_type'] != '']
    print(f"ì´ìƒ í˜„ìƒ ë°œê²¬ ìˆ˜: {len(actual_anomalies)}")
    
    if len(df) > 0:
        print(f"ì´ìƒ í˜„ìƒ ë¹„ìœ¨: {len(actual_anomalies)/len(df)*100:.2f}%")
    
    # ë‚˜ë…¸ì´ˆ ì •ë°€ë„ í†µê³„
    if 'true_critical_section_nanoTime_start' in df.columns:
        nano_count = df['true_critical_section_nanoTime_start'].notna().sum()
        print(f"ë‚˜ë…¸ì´ˆ ì •ë°€ë„ ë°ì´í„°: {nano_count}ê±´ ({nano_count/len(df)*100:.1f}%)")
    
    if len(actual_anomalies) > 0:
        print("\n=== 4ê°€ì§€ ê·œì¹™ë³„ ì´ìƒ í˜„ìƒ ë¶„í¬ ===")
        
        error_counts = {
            'ê°’ ë¶ˆì¼ì¹˜': 0,
            'ê²½í•© ë°œìƒ ì˜¤ë¥˜': 0,
            'ì •ì› ì´ˆê³¼ ì˜¤ë¥˜': 0,
            'ìƒíƒœ ì „ì´ ì˜¤ë¥˜': 0
        }
        
        for anomaly_str in actual_anomalies['anomaly_type']:
            for error_type in error_counts.keys():
                if error_type in anomaly_str:
                    error_counts[error_type] += 1
        
        for error_type, count in error_counts.items():
            percentage = count/len(actual_anomalies)*100 if len(actual_anomalies) > 0 else 0
            print(f"  - {error_type}: {count}ê±´ ({percentage:.1f}%)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Race Condition ë¶„ì„ê¸° (ìˆ˜ì •ëœ ë²„ì „)")
    parser.add_argument('input_csv', help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('output_csv', help='ì¶œë ¥ CSV íŒŒì¼')
    parser.add_argument('--detailed_output', default='detailed_analysis.txt', help='ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ íŒŒì¼')
    parser.add_argument('--rooms', help='ë¶„ì„í•  ë°© ë²ˆí˜¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
    parser.add_argument('--xlsx_output', help='Excel ì¶œë ¥ íŒŒì¼ (ì„ íƒì‚¬í•­)')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ Race Condition ë¶„ì„ê¸° ì‹œì‘...")
        
        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(args.input_csv)
        print(f"âœ… CSV íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['roomNumber', 'bin', 'user_id', 'prev_people', 'curr_people', 'expected_people', 
                           'max_people', 'prev_entry_time', 'curr_entry_time', 'room_entry_sequence']
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            return
        
        print("âœ… í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ì™„ë£Œ")
        
        # ë°© ë²ˆí˜¸ í•„í„°ë§
        if args.rooms:
            room_numbers = [int(room.strip()) for room in args.rooms.split(',')]
            df = df[df['roomNumber'].isin(room_numbers)]
            print(f"ğŸ” ë°© ë²ˆí˜¸ {room_numbers}ë¡œ í•„í„°ë§: {len(df)} ë ˆì½”ë“œ")
        
        # ë©”ì¸ ë¶„ì„ ì‹¤í–‰
        all_records, detailed_analysis = detect_race_condition_anomalies(df)
        
        # ê²°ê³¼ DataFrame ìƒì„± (ëª¨ë“  ë ˆì½”ë“œ í¬í•¨)
        result_df = pd.DataFrame(all_records)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        basic_cols = ['roomNumber', 'bin', 'user_id', 'prev_people', 'curr_people', 'expected_people', 
                     'max_people', 'prev_entry_time', 'curr_entry_time', 
                     'true_critical_section_nanoTime_start', 'true_critical_section_nanoTime_end',
                     'anomaly_type', 'room_entry_sequence']
        
        detail_cols = ['lost_update_expected', 'lost_update_actual', 'lost_update_diff',
                      'contention_group_size', 'contention_user_ids',
                      'over_capacity_amount', 'over_capacity_curr', 'over_capacity_max',
                      'expected_curr_by_sequence', 'actual_curr_people', 'curr_sequence_diff',
                      'sorted_sequence_position',
                      'intervening_users_in_critical_section', 'intervening_user_count_critical',
                      'true_critical_section_duration_nanos']
        
        # ëª¨ë“  ì»¬ëŸ¼ í¬í•¨ (32ê°œ ì»¬ëŸ¼)
        all_cols = basic_cols + detail_cols
        existing_cols = [col for col in all_cols if col in result_df.columns]
        result_df = result_df[existing_cols]
        result_df = result_df.fillna('')
        
        # CSV ì €ì¥
        result_df.to_csv(args.output_csv, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì „ì²´ ê²°ê³¼ {len(result_df)}ê°œê°€ {args.output_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # Excel ì €ì¥ (í•­ìƒ ìƒì„±)
        excel_filename = args.xlsx_output if args.xlsx_output else args.output_csv.replace('.csv', '.xlsx')
        
        # Excel ì €ì¥ ì „ì— íƒ€ì„ì¡´ ì •ë³´ ì œê±°
        excel_df = result_df.copy()
        if 'prev_entry_time' in excel_df.columns:
            excel_df['prev_entry_time'] = pd.to_datetime(excel_df['prev_entry_time']).dt.tz_localize(None)
        if 'curr_entry_time' in excel_df.columns:
            excel_df['curr_entry_time'] = pd.to_datetime(excel_df['curr_entry_time']).dt.tz_localize(None)
        
        excel_df.to_excel(excel_filename, index=False)
        print(f"ğŸ“Š Excel íŒŒì¼ ì €ì¥ë¨: {excel_filename}")
        
        # ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ì €ì¥ (ì´ìƒí˜„ìƒì´ ìˆëŠ” ê²½ìš°ë§Œ)
        with open(args.detailed_output, 'w', encoding='utf-8') as f:
            f.write("Race Condition ìƒì„¸ ë¶„ì„ ê²°ê³¼\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì…ë ¥ íŒŒì¼: {args.input_csv}\n")
            f.write(f"ì´ ì´ìƒ í˜„ìƒ ìˆ˜: {len(detailed_analysis)}\n\n")
            
            for detailed_text in detailed_analysis:
                f.write(detailed_text)
                f.write("\n")
        
        print(f"ğŸ“„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {args.detailed_output}")
        
        # í†µê³„ ì¶œë ¥
        print_statistics(df, result_df)
        
        print("\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()