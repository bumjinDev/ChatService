#!/usr/bin/env python3
"""
Race Condition ì „ì²´ í†µí•© í†µê³„ ë¶„ì„ê¸°
- ì´ìƒí˜„ìƒ íƒì§€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 4ê°€ì§€ ê·œì¹™ë³„ ì „ì²´ í†µí•© í†µê³„ì  ì§‘ê³„ ë¶„ì„
- ë°©/bin êµ¬ë¶„ ì—†ì´ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë°œìƒë¥ , ì‹¬ê°ë„, ë¶„í¬ ë¶„ì„ ì œê³µ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import argparse
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, PatternFill, Alignment
import traceback

def load_and_validate_data(preprocessor_file, analysis_file):
    """ë°ì´í„° ë¡œë“œ ë° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦"""
    print("ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    # ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ
    preprocessor_df = pd.read_csv(preprocessor_file)
    print(f"âœ… ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(preprocessor_df)}í–‰")
    
    # ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„° ë¡œë“œ
    analysis_df = pd.read_csv(analysis_file)
    print(f"âœ… ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(analysis_df)}í–‰")
    
    # ì „ì²˜ë¦¬ ë°ì´í„° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    preprocessor_required = ['roomNumber', 'bin', 'user_id']
    missing_preprocessor = [col for col in preprocessor_required if col not in preprocessor_df.columns]
    if missing_preprocessor:
        raise ValueError(f"ì „ì²˜ë¦¬ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_preprocessor}")
    
    # ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    analysis_required = ['roomNumber', 'bin', 'anomaly_type', 'lost_update_diff', 
                        'contention_group_size', 'over_capacity_amount', 'curr_sequence_diff']
    missing_analysis = [col for col in analysis_required if col not in analysis_df.columns]
    if missing_analysis:
        raise ValueError(f"ì´ìƒí˜„ìƒ ë¶„ì„ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_analysis}")
    
    # ë°ì´í„° íƒ€ì… ì •ë¦¬ (NaN/ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬)
    preprocessor_df['roomNumber'] = pd.to_numeric(preprocessor_df['roomNumber'], errors='coerce').fillna(0).astype(int)
    preprocessor_df['bin'] = pd.to_numeric(preprocessor_df['bin'], errors='coerce').fillna(0).astype(int)
    
    analysis_df['roomNumber'] = pd.to_numeric(analysis_df['roomNumber'], errors='coerce').fillna(0).astype(int)
    analysis_df['bin'] = pd.to_numeric(analysis_df['bin'], errors='coerce').fillna(0).astype(int)
    
    print("âœ… ë°ì´í„° ê²€ì¦ ë° íƒ€ì… ë³€í™˜ ì™„ë£Œ")
    return preprocessor_df, analysis_df

def calculate_total_requests(preprocessor_df):
    """ì „ì²´ ìš”ì²­ìˆ˜ ì§‘ê³„ (ëª¨ë“  ë°©ê³¼ bin í†µí•©)"""
    print("ğŸ“Š ì „ì²´ ìš”ì²­ìˆ˜ ì§‘ê³„ ì¤‘...")
    
    total_requests = len(preprocessor_df)
    total_rooms = preprocessor_df['roomNumber'].nunique()
    total_bins = len(preprocessor_df.groupby(['roomNumber', 'bin']))
    
    print(f"âœ… ì§‘ê³„ ì™„ë£Œ:")
    print(f"  - ì „ì²´ ìš”ì²­ìˆ˜: {total_requests:,}ê±´")
    print(f"  - ì „ì²´ ë°© ìˆ˜: {total_rooms}ê°œ")
    print(f"  - ì „ì²´ (ë°©Ã—bin) ì¡°í•©: {total_bins}ê°œ")
    
    return {
        'total_requests': total_requests,
        'total_rooms': total_rooms,
        'total_bins': total_bins
    }

def calculate_overall_statistics(filtered_df, value_column, total_info, rule_name, use_absolute=False):
    """ì „ì²´ í†µí•© í†µê³„ ê³„ì‚° í•¨ìˆ˜"""
    print(f"  ğŸ“ˆ {rule_name} í†µê³„ ê³„ì‚° ì¤‘...")
    
    # ê¸°ë³¸ ì •ë³´
    total_requests = total_info['total_requests']
    total_rooms = total_info['total_rooms']
    total_bins = total_info['total_bins']
    
    # ì´ˆê¸° ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    result = {
        'ë¶„ì„ êµ¬ë¶„': rule_name,
        'ì „ì²´ ìš”ì²­ìˆ˜': total_requests,
        'ì „ì²´ ë°© ìˆ˜': total_rooms,
        'ì „ì²´ (ë°©Ã—bin) ì¡°í•©ìˆ˜': total_bins,
        'ë°œìƒ ê±´ìˆ˜': 0,
        'ë°œìƒë¥  (%)': 0.0,
        'ì˜í–¥ë°›ì€ ë°© ìˆ˜': 0,
        'ì˜í–¥ë°›ì€ (ë°©Ã—bin) ì¡°í•©ìˆ˜': 0,
        'ë°©ë³„ í‰ê·  ë°œìƒë¥  (%)': 0.0,
        'binë³„ í‰ê·  ë°œìƒë¥  (%)': 0.0,
        'ì´í•© ê°’': 0.0,
        'í‰ê·  ê°’': np.nan,
        'ìµœì†Œ ê°’': np.nan,
        'ìµœëŒ€ ê°’': np.nan,
        'ì¤‘ê°„ ê°’': np.nan,
        'í‘œì¤€í¸ì°¨ ê°’': 0.0
    }
    
    # ì´ìƒí˜„ìƒì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤ì œ í†µê³„ ê³„ì‚°
    if len(filtered_df) > 0:
        # í•´ë‹¹ ê°’ë“¤ ì¶”ì¶œ (NaN ì œê±°)
        values = filtered_df[value_column].dropna()
        
        if len(values) > 0:
            # ê¸°ë³¸ ë°œìƒ í†µê³„
            occurrence_count = len(values)
            occurrence_rate = (occurrence_count / total_requests * 100) if total_requests > 0 else 0
            
            # ì˜í–¥ë°›ì€ ë°©ê³¼ bin ì¡°í•© ê³„ì‚°
            affected_rooms = filtered_df['roomNumber'].nunique()
            affected_bins = len(filtered_df.groupby(['roomNumber', 'bin']))
            
            # ë°©ë³„ ë°œìƒë¥  í‰ê·  ê³„ì‚°
            room_stats = filtered_df.groupby('roomNumber').size().reset_index(name='room_count')
            # ì „ì²´ ë°©ë³„ ìš”ì²­ìˆ˜ ê³„ì‚° (ì „ì²˜ë¦¬ ë°ì´í„°ì—ì„œ)
            if 'preprocessor_df' in globals():
                room_requests = preprocessor_df.groupby('roomNumber').size().reset_index(name='total_room_requests')
                room_stats = room_stats.merge(room_requests, on='roomNumber', how='left')
                room_stats['room_rate'] = (room_stats['room_count'] / room_stats['total_room_requests'] * 100)
                avg_room_rate = room_stats['room_rate'].mean()
            else:
                avg_room_rate = 0.0
            
            # binë³„ ë°œìƒë¥  í‰ê·  ê³„ì‚°
            bin_stats = filtered_df.groupby(['roomNumber', 'bin']).size().reset_index(name='bin_count')
            if 'preprocessor_df' in globals():
                bin_requests = preprocessor_df.groupby(['roomNumber', 'bin']).size().reset_index(name='total_bin_requests')
                bin_stats = bin_stats.merge(bin_requests, on=['roomNumber', 'bin'], how='left')
                bin_stats['bin_rate'] = (bin_stats['bin_count'] / bin_stats['total_bin_requests'] * 100)
                avg_bin_rate = bin_stats['bin_rate'].mean()
            else:
                avg_bin_rate = 0.0
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            result['ë°œìƒ ê±´ìˆ˜'] = int(occurrence_count)
            result['ë°œìƒë¥  (%)'] = round(occurrence_rate, 2)
            result['ì˜í–¥ë°›ì€ ë°© ìˆ˜'] = int(affected_rooms)
            result['ì˜í–¥ë°›ì€ (ë°©Ã—bin) ì¡°í•©ìˆ˜'] = int(affected_bins)
            result['ë°©ë³„ í‰ê·  ë°œìƒë¥  (%)'] = round(avg_room_rate, 2)
            result['binë³„ í‰ê·  ë°œìƒë¥  (%)'] = round(avg_bin_rate, 2)
            
            # ê°’ í†µê³„ ê³„ì‚° (ì ˆëŒ“ê°’ ì˜µì…˜ ì ìš©)
            if use_absolute:
                result['ì´í•© ê°’'] = round(values.abs().sum(), 2)
                result['í‰ê·  ê°’'] = round(values.abs().mean(), 2)
            else:
                result['ì´í•© ê°’'] = round(values.sum(), 2)
                result['í‰ê·  ê°’'] = round(values.mean(), 2)
            
            result['ìµœì†Œ ê°’'] = round(values.min(), 2)
            result['ìµœëŒ€ ê°’'] = round(values.max(), 2)
            result['ì¤‘ê°„ ê°’'] = round(values.median(), 2)
            result['í‘œì¤€í¸ì°¨ ê°’'] = round(values.std(), 4) if len(values) > 1 else 0.0
    
    return result

def analyze_lost_update(analysis_df, total_info):
    """ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ ì „ì²´ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update) ì „ì²´ ë¶„ì„ ì¤‘...")
    
    # 'ê°’ ë¶ˆì¼ì¹˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ê°’ ë¶ˆì¼ì¹˜', na=False)]
    print(f"  - ê°’ ë¶ˆì¼ì¹˜ ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # lost_update_diff ê¸°ì¤€ í†µê³„ ê³„ì‚°
    result = calculate_overall_statistics(filtered_df, 'lost_update_diff', total_info, "ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update)")
    
    # íŠ¹í™”ëœ ì»¬ëŸ¼ëª… ì ìš©
    specialized_mapping = {
        'ì´í•© ê°’': 'ì˜¤ì°¨ ëˆ„ì  ì´í•©',
        'í‰ê·  ê°’': 'í‰ê·  ì˜¤ì°¨',
        'ìµœì†Œ ê°’': 'ìµœì†Œ ì˜¤ì°¨',
        'ìµœëŒ€ ê°’': 'ìµœëŒ€ ì˜¤ì°¨',
        'ì¤‘ê°„ ê°’': 'ì¤‘ê°„ê°’ ì˜¤ì°¨',
        'í‘œì¤€í¸ì°¨ ê°’': 'ì˜¤ì°¨ í‘œì¤€í¸ì°¨'
    }
    
    for old_key, new_key in specialized_mapping.items():
        if old_key in result:
            result[new_key] = result.pop(old_key)
    
    print("âœ… ê°’ ë¶ˆì¼ì¹˜ ì „ì²´ ë¶„ì„ ì™„ë£Œ")
    return result

def analyze_contention(analysis_df, total_info):
    """ê·œì¹™ 2: ê²½í•© ë°œìƒ ì „ì²´ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 2: ê²½í•© ë°œìƒ (Contention) ì „ì²´ ë¶„ì„ ì¤‘...")
    
    # 'ê²½í•© ë°œìƒ ì˜¤ë¥˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ê²½í•© ë°œìƒ ì˜¤ë¥˜', na=False)]
    print(f"  - ê²½í•© ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # contention_group_size ê¸°ì¤€ í†µê³„ ê³„ì‚°
    result = calculate_overall_statistics(filtered_df, 'contention_group_size', total_info, "ê·œì¹™ 2: ê²½í•© ë°œìƒ (Contention)")
    
    # íŠ¹í™”ëœ ì»¬ëŸ¼ëª… ì ìš©
    specialized_mapping = {
        'ì´í•© ê°’': 'ì´ ê²½í•© ìŠ¤ë ˆë“œ ìˆ˜',
        'í‰ê·  ê°’': 'í‰ê·  ê²½í•© ìŠ¤ë ˆë“œ ìˆ˜',
        'ìµœì†Œ ê°’': 'ìµœì†Œ ê²½í•© ê·¸ë£¹ í¬ê¸°',
        'ìµœëŒ€ ê°’': 'ìµœëŒ€ ê²½í•© ìŠ¤ë ˆë“œ ìˆ˜',
        'ì¤‘ê°„ ê°’': 'ì¤‘ê°„ê°’ ê²½í•© ê·¸ë£¹ í¬ê¸°',
        'í‘œì¤€í¸ì°¨ ê°’': 'ê²½í•© ê°•ë„ í‘œì¤€í¸ì°¨'
    }
    
    for old_key, new_key in specialized_mapping.items():
        if old_key in result:
            result[new_key] = result.pop(old_key)
    
    print("âœ… ê²½í•© ë°œìƒ ì „ì²´ ë¶„ì„ ì™„ë£Œ")
    return result

def analyze_capacity_exceeded(analysis_df, total_info):
    """ê·œì¹™ 3: ì •ì› ì´ˆê³¼ ì „ì²´ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 3: ì •ì› ì´ˆê³¼ (Capacity Exceeded) ì „ì²´ ë¶„ì„ ì¤‘...")
    
    # 'ì •ì› ì´ˆê³¼ ì˜¤ë¥˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ì •ì› ì´ˆê³¼ ì˜¤ë¥˜', na=False)]
    print(f"  - ì •ì› ì´ˆê³¼ ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # over_capacity_amount ê¸°ì¤€ í†µê³„ ê³„ì‚°
    result = calculate_overall_statistics(filtered_df, 'over_capacity_amount', total_info, "ê·œì¹™ 3: ì •ì› ì´ˆê³¼ (Capacity Exceeded)")
    
    # íŠ¹í™”ëœ ì»¬ëŸ¼ëª… ì ìš©
    specialized_mapping = {
        'ì´í•© ê°’': 'ì´ ì´ˆê³¼ ì¸ì›',
        'í‰ê·  ê°’': 'í‰ê·  ì´ˆê³¼ ì¸ì›',
        'ìµœì†Œ ê°’': 'ìµœì†Œ ì´ˆê³¼ ì¸ì›',
        'ìµœëŒ€ ê°’': 'ìµœëŒ€ ì´ˆê³¼ ì¸ì›',
        'ì¤‘ê°„ ê°’': 'ì¤‘ê°„ê°’ ì´ˆê³¼ ì¸ì›',
        'í‘œì¤€í¸ì°¨ ê°’': 'ì´ˆê³¼ ê·œëª¨ í‘œì¤€í¸ì°¨'
    }
    
    for old_key, new_key in specialized_mapping.items():
        if old_key in result:
            result[new_key] = result.pop(old_key)
    
    print("âœ… ì •ì› ì´ˆê³¼ ì „ì²´ ë¶„ì„ ì™„ë£Œ")
    return result

def analyze_state_transition(analysis_df, total_info):
    """ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ì „ì²´ í†µí•© ë¶„ì„"""
    print("ğŸ” ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (State Transition) ì „ì²´ ë¶„ì„ ì¤‘...")
    
    # 'ìƒíƒœ ì „ì´ ì˜¤ë¥˜' í¬í•¨ëœ ë ˆì½”ë“œ í•„í„°ë§
    filtered_df = analysis_df[analysis_df['anomaly_type'].str.contains('ìƒíƒœ ì „ì´ ì˜¤ë¥˜', na=False)]
    print(f"  - ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ë°œìƒ ë ˆì½”ë“œ: {len(filtered_df)}ê±´")
    
    # curr_sequence_diff ê¸°ì¤€ í†µê³„ ê³„ì‚° (ì ˆëŒ“ê°’ ì‚¬ìš©)
    result = calculate_overall_statistics(filtered_df, 'curr_sequence_diff', total_info, "ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (State Transition)", use_absolute=True)
    
    # ìƒíƒœ ì „ì´ ì˜¤ë¥˜ëŠ” ê¸°ë³¸ ì»¬ëŸ¼ëª… ìœ ì§€ (ë‹¤ë¥¸ ê·œì¹™ë“¤ê³¼ ë™ì¼í•˜ê²Œ)
    
    print("âœ… ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ì „ì²´ ë¶„ì„ ì™„ë£Œ")
    return result

def create_individual_dataframes(lost_update_result, contention_result, capacity_result, state_transition_result):
    """4ê°œ ê·œì¹™ ê²°ê³¼ë¥¼ ê°ê°ì˜ DataFrameìœ¼ë¡œ ìƒì„±"""
    print("ğŸ“Š ê·œì¹™ë³„ ê°œë³„ DataFrame ìƒì„± ì¤‘...")
    
    # ê° ê·œì¹™ë³„ DataFrame ìƒì„±
    lost_update_df = pd.DataFrame([lost_update_result])
    contention_df = pd.DataFrame([contention_result])
    capacity_df = pd.DataFrame([capacity_result])
    state_transition_df = pd.DataFrame([state_transition_result])
    
    print("âœ… ê°œë³„ DataFrame ìƒì„± ì™„ë£Œ")
    return lost_update_df, contention_df, capacity_df, state_transition_df

def add_dataframe_to_sheet(ws, df, sheet_title):
    """DataFrameì„ ì›Œí¬ì‹œíŠ¸ì— ì¶”ê°€í•˜ê³  ìŠ¤íƒ€ì¼ ì ìš©"""
    # ì‹œíŠ¸ ì œëª© ì¶”ê°€
    ws['A1'] = sheet_title
    ws['A1'].font = Font(size=16, bold=True)
    ws['A1'].fill = PatternFill(start_color="2F5597", end_color="2F5597", fill_type="solid")
    ws['A1'].font = Font(size=16, bold=True, color="FFFFFF")
    
    # ë¹ˆ í–‰ ì¶”ê°€
    start_row = 3
    
    # ì»¬ëŸ¼ í—¤ë” ì¶”ê°€
    for col_idx, column in enumerate(df.columns, 1):
        cell = ws.cell(row=start_row, column=col_idx, value=column)
        cell.font = Font(bold=True, size=11)
        cell.fill = PatternFill(start_color="B7D7F1", end_color="B7D7F1", fill_type="solid")
        cell.alignment = Alignment(horizontal='center', vertical='center')
    
    # ë°ì´í„° ì¶”ê°€
    for row_idx, row in enumerate(df.itertuples(index=False), start_row + 1):
        for col_idx, value in enumerate(row, 1):
            cell = ws.cell(row=row_idx, column=col_idx, value=value)
            
            # ë¶„ì„ êµ¬ë¶„ ì»¬ëŸ¼ì€ ê°•ì¡° ì²˜ë¦¬
            if col_idx == 1:  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ (ë¶„ì„ êµ¬ë¶„)
                cell.font = Font(bold=True)
                cell.fill = PatternFill(start_color="E7F3FF", end_color="E7F3FF", fill_type="solid")
    
    # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •
    for col_idx, column in enumerate(df.columns, 1):
        max_length = max(len(str(column)), 12)
        if len(df) > 0:
            col_values = [str(val) for val in df.iloc[:, col_idx-1] if pd.notna(val)]
            if col_values:
                max_length = max(max_length, max(len(val) for val in col_values))
        ws.column_dimensions[get_column_letter(col_idx)].width = min(max_length + 3, 35)

def create_excel_output(lost_update_df, contention_df, capacity_df, state_transition_df, output_file):
    """4ê°œ ì‹œíŠ¸ë¡œ êµ¬ì„±ëœ Excel íŒŒì¼ ìƒì„± (ê° ê·œì¹™ë³„ ê°œë³„ ì‹œíŠ¸)"""
    print("ğŸ“Š Excel íŒŒì¼ ìƒì„± ì¤‘...")
    
    wb = Workbook()
    
    # ê¸°ë³¸ ì‹œíŠ¸ ì œê±°
    wb.remove(wb.active)
    
    # ì‹œíŠ¸ 1: Lost Update ì „ì²´ ë¶„ì„
    ws1 = wb.create_sheet("Overall_LostUpdate")
    add_dataframe_to_sheet(ws1, lost_update_df, "ì „ì²´ í†µí•©: ê·œì¹™ 1 - ê°’ ë¶ˆì¼ì¹˜ (Lost Update) ë¶„ì„")
    
    # ì‹œíŠ¸ 2: Contention ì „ì²´ ë¶„ì„
    ws2 = wb.create_sheet("Overall_Contention") 
    add_dataframe_to_sheet(ws2, contention_df, "ì „ì²´ í†µí•©: ê·œì¹™ 2 - ê²½í•© ë°œìƒ (Contention) ë¶„ì„")
    
    # ì‹œíŠ¸ 3: Capacity Exceeded ì „ì²´ ë¶„ì„
    ws3 = wb.create_sheet("Overall_Capacity")
    add_dataframe_to_sheet(ws3, capacity_df, "ì „ì²´ í†µí•©: ê·œì¹™ 3 - ì •ì› ì´ˆê³¼ (Capacity Exceeded) ë¶„ì„")
    
    # ì‹œíŠ¸ 4: State Transition ì „ì²´ ë¶„ì„
    ws4 = wb.create_sheet("Overall_StateTransition")
    add_dataframe_to_sheet(ws4, state_transition_df, "ì „ì²´ í†µí•©: ê·œì¹™ 4 - ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (State Transition) ë¶„ì„")
    
    wb.save(output_file)
    print(f"âœ… Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    print("  ğŸ“‹ ìƒì„±ëœ ì‹œíŠ¸:")
    print("    - Overall_LostUpdate: ê°’ ë¶ˆì¼ì¹˜ ì „ì²´ ë¶„ì„")
    print("    - Overall_Contention: ê²½í•© ë°œìƒ ì „ì²´ ë¶„ì„") 
    print("    - Overall_Capacity: ì •ì› ì´ˆê³¼ ì „ì²´ ë¶„ì„")
    print("    - Overall_StateTransition: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ ì „ì²´ ë¶„ì„")

def print_summary_statistics(lost_update_df, contention_df, capacity_df, state_transition_df):
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ í†µê³„ ì¶œë ¥"""
    print("\n" + "="*90)
    print("ğŸ“ˆ RACE CONDITION ì „ì²´ í†µí•© ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("="*90)
    
    # ê° DataFrameì—ì„œ ì •ë³´ ì¶”ì¶œ
    dataframes = [
        ("ê·œì¹™ 1: ê°’ ë¶ˆì¼ì¹˜ (Lost Update)", lost_update_df),
        ("ê·œì¹™ 2: ê²½í•© ë°œìƒ (Contention)", contention_df),
        ("ê·œì¹™ 3: ì •ì› ì´ˆê³¼ (Capacity Exceeded)", capacity_df),
        ("ê·œì¹™ 4: ìƒíƒœ ì „ì´ ì˜¤ë¥˜ (State Transition)", state_transition_df)
    ]
    
    # ì²« ë²ˆì§¸ ê·œì¹™ì—ì„œ ì „ì²´ ì •ë³´ ì¶”ì¶œ
    if len(lost_update_df) > 0:
        first_row = lost_update_df.iloc[0]
        total_requests = first_row['ì „ì²´ ìš”ì²­ìˆ˜']
        total_rooms = first_row['ì „ì²´ ë°© ìˆ˜']
        total_bins = first_row['ì „ì²´ (ë°©Ã—bin) ì¡°í•©ìˆ˜']
        
        print(f"ì „ì²´ ë¶„ì„ ëŒ€ìƒ:")
        print(f"  - ìš”ì²­ ìˆ˜: {total_requests:,}ê±´")
        print(f"  - ë°© ìˆ˜: {total_rooms}ê°œ")
        print(f"  - (ë°©Ã—bin) ì¡°í•©: {total_bins}ê°œ")
    
    print(f"\n--- ê·œì¹™ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ---")
    
    total_anomaly_requests = 0
    
    for rule_name, df in dataframes:
        if len(df) > 0:
            row = df.iloc[0]
            occurrence_count = row['ë°œìƒ ê±´ìˆ˜']
            occurrence_rate = row['ë°œìƒë¥  (%)']
            affected_rooms = row['ì˜í–¥ë°›ì€ ë°© ìˆ˜']
            affected_bins = row['ì˜í–¥ë°›ì€ (ë°©Ã—bin) ì¡°í•©ìˆ˜']
            
            print(f"\n{rule_name}:")
            print(f"  - ë°œìƒ ê±´ìˆ˜: {occurrence_count:,}ê±´ ({occurrence_rate}%)")
            print(f"  - ì˜í–¥ë°›ì€ ë°©: {affected_rooms}ê°œ")
            print(f"  - ì˜í–¥ë°›ì€ bin: {affected_bins}ê°œ")
            
            total_anomaly_requests += occurrence_count
        else:
            print(f"\n{rule_name}: ë°ì´í„° ì—†ìŒ")
    
    if len(lost_update_df) > 0:
        overall_anomaly_rate = (total_anomaly_requests / total_requests * 100) if total_requests > 0 else 0
        
        print(f"\n--- ì „ì²´ ìš”ì•½ ---")
        print(f"ì „ì²´ ì´ìƒí˜„ìƒ ë°œìƒ: {total_anomaly_requests:,}ê±´")
        print(f"ì „ì²´ ì´ìƒí˜„ìƒ ë°œìƒë¥ : {overall_anomaly_rate:.2f}%")
        print(f"ì •ìƒ ìš”ì²­: {total_requests - total_anomaly_requests:,}ê±´ ({100 - overall_anomaly_rate:.2f}%)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Race Condition ì „ì²´ í†µí•© í†µê³„ ë¶„ì„ê¸°")
    parser.add_argument('preprocessor_csv', help='ì „ì²˜ë¦¬ ê²°ê³¼ CSV íŒŒì¼')
    parser.add_argument('analysis_csv', help='ì´ìƒí˜„ìƒ ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼')
    parser.add_argument('output_xlsx', help='ì „ì²´ í†µí•© ë¶„ì„ Excel ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--rooms', help='ë¶„ì„í•  ë°© ë²ˆí˜¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ Race Condition ì „ì²´ í†µí•© ë¶„ì„ê¸° ì‹œì‘...")
        print(f"ì…ë ¥ íŒŒì¼ 1: {args.preprocessor_csv}")
        print(f"ì…ë ¥ íŒŒì¼ 2: {args.analysis_csv}")
        print(f"ì¶œë ¥ íŒŒì¼: {args.output_xlsx}")
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        preprocessor_df, analysis_df = load_and_validate_data(args.preprocessor_csv, args.analysis_csv)
        
        # ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì • (í†µê³„ ê³„ì‚° í•¨ìˆ˜ì—ì„œ ì°¸ì¡°í•˜ê¸° ìœ„í•´)
        globals()['preprocessor_df'] = preprocessor_df
        
        # 2. ë°© ë²ˆí˜¸ í•„í„°ë§ (ì„ íƒì‚¬í•­)
        if args.rooms:
            room_numbers = [int(room.strip()) for room in args.rooms.split(',')]
            preprocessor_df = preprocessor_df[preprocessor_df['roomNumber'].isin(room_numbers)]
            analysis_df = analysis_df[analysis_df['roomNumber'].isin(room_numbers)]
            globals()['preprocessor_df'] = preprocessor_df  # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
            print(f"ğŸ” ë°© ë²ˆí˜¸ {room_numbers}ë¡œ í•„í„°ë§ ì ìš©")
        
        # 3. ì „ì²´ ìš”ì²­ ì •ë³´ ì§‘ê³„
        total_info = calculate_total_requests(preprocessor_df)
        
        # 4. 4ê°€ì§€ ê·œì¹™ë³„ ì „ì²´ í†µí•© ë¶„ì„
        lost_update_result = analyze_lost_update(analysis_df, total_info)
        contention_result = analyze_contention(analysis_df, total_info)
        capacity_result = analyze_capacity_exceeded(analysis_df, total_info)
        state_transition_result = analyze_state_transition(analysis_df, total_info)
        
        # 5. ê°œë³„ ê·œì¹™ë³„ DataFrame ìƒì„±
        lost_update_df, contention_df, capacity_df, state_transition_df = create_individual_dataframes(
            lost_update_result, contention_result, capacity_result, state_transition_result)
        
        # 6. Excel íŒŒì¼ ìƒì„± (4ê°œ ì‹œíŠ¸ë¡œ ë¶„ë¦¬)
        create_excel_output(lost_update_df, contention_df, capacity_df, state_transition_df, args.output_xlsx)
        
        # 7. ìš”ì•½ í†µê³„ ì¶œë ¥
        print_summary_statistics(lost_update_df, contention_df, capacity_df, state_transition_df)
        
        print("\nğŸ‰ ì „ì²´ í†µí•© ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    main()